{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c989c9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported helpers from services.signals.hmm_features\n"
     ]
    }
   ],
   "source": [
    "# Project helpers: prefer importing from src module; fallback to in-notebook definitions\n",
    "try:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.abspath(\"/home/jordan/oryx/code/repos/fks/src/python\"))\n",
    "    from services.signals.hmm_features import (\n",
    "        load_price_data, build_features, fit_hmm_and_states, build_signals, objective\n",
    "    )\n",
    "    print(\"Imported helpers from services.signals.hmm_features\")\n",
    "except Exception as e:\n",
    "    print(\"Module import failed, using in-notebook helpers. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ab49dd",
   "metadata": {},
   "source": [
    "# Refactor: Data fetching, timeframes, and HMM → Transformer pipeline\n",
    "\n",
    "This notebook now has clearly separated steps:\n",
    "\n",
    "- Parameters: symbol, timeframe (1m, 5m, 15m, 1h, 4h, 1d), date range, and data source.\n",
    "- Data fetching (own cell): loads and resamples OHLCV to selected timeframe.\n",
    "- Feature engineering: returns/volatility features for HMM.\n",
    "- HMM training: fit GaussianHMM, infer states, and compute/visualize transition matrix.\n",
    "- Transformer prep: enrich dataset with HMM state features (one-hot/embedding-ready) for downstream transformer models.\n",
    "\n",
    "Run cells in order. Adjust only the parameters cell to change symbol/timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9b363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies ready.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies: ensure required packages are installed in this kernel\n",
    "import importlib, sys, subprocess\n",
    "\n",
    "def ensure(pkg, import_name=None):\n",
    "    name = import_name or pkg\n",
    "    try:\n",
    "        return importlib.import_module(name)\n",
    "    except Exception:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg])\n",
    "        return importlib.import_module(name)\n",
    "\n",
    "pd = ensure(\"pandas\", \"pandas\")\n",
    "np = ensure(\"numpy\", \"numpy\")\n",
    "plt = ensure(\"matplotlib\", \"matplotlib.pyplot\")\n",
    "sns = ensure(\"seaborn\", \"seaborn\")\n",
    "sklearn = ensure(\"scikit-learn\", \"sklearn\")\n",
    "hmmlearn_mod = ensure(\"hmmlearn\", \"hmmlearn\")\n",
    "yf = ensure(\"yfinance\", \"yfinance\")\n",
    "\n",
    "print(\"Dependencies ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed2676",
   "metadata": {},
   "source": [
    "## Simple Up/Down Transition Matrix (yfinance example)\n",
    "\n",
    "This optional section replicates a simple two-state transition analysis using yfinance data. It labels each day as up/down based on daily returns and computes the transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yfinance up/down transition analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "YF_TICKER = \"GC=F\"  # Comex Gold futures (continuous)\n",
    "YF_START = \"2010-01-01\"\n",
    "YF_END = \"2022-04-22\"\n",
    "\n",
    "yf_df = yf.download(YF_TICKER, start=YF_START, end=YF_END, auto_adjust=False, progress=False)\n",
    "\n",
    "# Safety checks\n",
    "if yf_df.empty:\n",
    "    raise RuntimeError(\"yfinance download returned empty dataframe. Check ticker/date range.\")\n",
    "\n",
    "# Inspect\n",
    "display(yf_df.tail())\n",
    "\n",
    "# Label up/down using Adj Close when available, else Close\n",
    "price_col = \"Adj Close\" if \"Adj Close\" in yf_df.columns else \"Close\"\n",
    "yf_df[\"daily_return\"] = yf_df[price_col].pct_change()\n",
    "yf_df[\"state\"] = np.where(yf_df[\"daily_return\"] >= 0, \"up\", \"down\")\n",
    "\n",
    "# Compute 2x2 transition matrix (up/down) using masks to avoid query issues\n",
    "mask_up = yf_df[\"state\"] == \"up\"\n",
    "mask_down = yf_df[\"state\"] == \"down\"\n",
    "up_total = int(mask_up.sum())\n",
    "down_total = int(mask_down.sum())\n",
    "\n",
    "up_to_up = (int((mask_up & (yf_df[\"state\"].shift(-1) == \"up\")).sum()) / up_total) if up_total else np.nan\n",
    "up_to_down = (int((mask_up & (yf_df[\"state\"].shift(-1) == \"down\")).sum()) / up_total) if up_total else np.nan\n",
    "\n",
    "down_to_up = (int((mask_down & (yf_df[\"state\"].shift(-1) == \"up\")).sum()) / down_total) if down_total else np.nan\n",
    "down_to_down = (int((mask_down & (yf_df[\"state\"].shift(-1) == \"down\")).sum()) / down_total) if down_total else np.nan\n",
    "\n",
    "transition_matrix_ud = pd.DataFrame({\n",
    "    \"up\": [up_to_up, up_to_down],\n",
    "    \"down\": [down_to_up, down_to_down]\n",
    "}, index=[\"up\", \"down\"]).round(6)\n",
    "\n",
    "print(transition_matrix_ud)\n",
    "\n",
    "# Example multi-step conditional probability (e.g., P(next is up | last 5 were down))\n",
    "cond_len = 5\n",
    "numerator = int(((yf_df[\"state\"] == \"up\")\n",
    "                 & (yf_df[\"state\"].shift(-1) == \"down\")\n",
    "                 & (yf_df[\"state\"].shift(-2) == \"down\")\n",
    "                 & (yf_df[\"state\"].shift(-3) == \"down\")\n",
    "                 & (yf_df[\"state\"].shift(-4) == \"down\")\n",
    "                 & (yf_df[\"state\"].shift(-5) == \"down\")).sum())\n",
    "\n",
    "denominator = int(((yf_df[\"state\"].shift(1) == \"down\")\n",
    "                   & (yf_df[\"state\"].shift(2) == \"down\")\n",
    "                   & (yf_df[\"state\"].shift(3) == \"down\")\n",
    "                   & (yf_df[\"state\"].shift(4) == \"down\")\n",
    "                   & (yf_df[\"state\"].shift(5) == \"down\")).sum())\n",
    "\n",
    "multi_step_prob = (numerator / denominator) if denominator else np.nan\n",
    "print(\"P(next=up | last 5 were down):\", multi_step_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617873bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SYMBOL': 'ES=F', 'TIMEFRAME': '5m', 'DATA_SOURCE': 'api', 'LOCAL_PATH': './data', 'START': None, 'END': None, 'N_STATES': 4, 'SEQ_LEN': 64, 'USE_RTH': True}\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Instrument and timeframe\n",
    "SYMBOL = \"ES=F\"            # E-mini S&P 500 Futures on Yahoo\n",
    "TIMEFRAME = \"5m\"           # focus on short-term trading: 1m/5m/15m\n",
    "\n",
    "# Data source control\n",
    "DATA_SOURCE = \"api\"         # 'local' | 'api' | 'auto'\n",
    "LOCAL_PATH = \"./data\"       # used if DATA_SOURCE is 'local' or 'auto'\n",
    "\n",
    "# Date range: for intraday with yfinance use period fallback; keep None\n",
    "START = None\n",
    "END = None\n",
    "\n",
    "# HMM configuration\n",
    "N_STATES = 4\n",
    "COVARIANCE_TYPE = \"full\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Transformer / feature preparation\n",
    "INCLUDE_ONE_HOT_STATES = True\n",
    "INCLUDE_STATE_PROBS = True\n",
    "SEQ_LEN = 64\n",
    "TARGET_COL = \"close\"\n",
    "\n",
    "# Session filtering (intraday)\n",
    "USE_RTH = True\n",
    "SESSION_TZ = \"America/New_York\"\n",
    "RTH_START = \"09:30\"\n",
    "RTH_END = \"16:00\"\n",
    "\n",
    "print({\n",
    "    'SYMBOL': SYMBOL,\n",
    "    'TIMEFRAME': TIMEFRAME,\n",
    "    'DATA_SOURCE': DATA_SOURCE,\n",
    "    'LOCAL_PATH': LOCAL_PATH,\n",
    "    'START': START,\n",
    "    'END': END,\n",
    "    'N_STATES': N_STATES,\n",
    "    'SEQ_LEN': SEQ_LEN,\n",
    "    'USE_RTH': USE_RTH,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c99f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data utilities: timeframe mapping, resampling, and fetch\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from typing import Literal, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# Map friendly timeframe to pandas offset alias (avoid deprecated 'T')\n",
    "TF_TO_PANDAS = {\n",
    "    \"1m\": \"1min\",\n",
    "    \"5m\": \"5min\",\n",
    "    \"15m\": \"15min\",\n",
    "    \"1h\": \"1H\",\n",
    "    \"4h\": \"4H\",\n",
    "    \"1d\": \"1D\",\n",
    "}\n",
    "\n",
    "# Map friendly timeframe to yfinance intervals\n",
    "YF_INTERVAL = {\n",
    "    \"1m\": \"1m\",\n",
    "    \"5m\": \"5m\",\n",
    "    \"15m\": \"15m\",\n",
    "    \"1h\": \"60m\",\n",
    "    \"4h\": \"60m\",  # fetch 60m and resample to 4H\n",
    "    \"1d\": \"1d\",\n",
    "}\n",
    "\n",
    "# Suggested default periods for intraday yfinance\n",
    "YF_PERIOD = {\n",
    "    \"1m\": \"7d\",\n",
    "    \"5m\": \"60d\",\n",
    "    \"15m\": \"60d\",\n",
    "    \"1h\": \"730d\",\n",
    "    \"4h\": \"730d\",\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize_timeframe(tf: str) -> str:\n",
    "    if tf not in TF_TO_PANDAS:\n",
    "        raise ValueError(f\"Unsupported timeframe: {tf}. Choose from {list(TF_TO_PANDAS)}\")\n",
    "    return TF_TO_PANDAS[tf]\n",
    "\n",
    "\n",
    "def _resample_ohlcv(df: pd.DataFrame, rule: str) -> pd.DataFrame:\n",
    "    # Expect columns: [open, high, low, close, volume]\n",
    "    ohlc = {\n",
    "        \"open\": \"first\",\n",
    "        \"high\": \"max\",\n",
    "        \"low\": \"min\",\n",
    "        \"close\": \"last\",\n",
    "        \"volume\": \"sum\",\n",
    "    }\n",
    "    return df.resample(rule).apply(ohlc).dropna()\n",
    "\n",
    "\n",
    "def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        if df.columns.nlevels > 1 and len(df.columns.get_level_values(-1).unique()) == 1:\n",
    "            df = df.copy()\n",
    "            df.columns = df.columns.droplevel(-1)\n",
    "        else:\n",
    "            df = df.copy()\n",
    "            df.columns = [\"_\".join([str(x) for x in tup if str(x) != \"\"]) for tup in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def _normalize_ohlcv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _flatten_columns(df)\n",
    "    df = df.rename(columns={\n",
    "        \"Open\": \"open\",\n",
    "        \"High\": \"high\",\n",
    "        \"Low\": \"low\",\n",
    "        \"Close\": \"close\",\n",
    "        \"Adj Close\": \"adj_close\",\n",
    "        \"Volume\": \"volume\",\n",
    "    })\n",
    "    cols = [c for c in [\"open\", \"high\", \"low\", \"close\", \"volume\"] if c in df.columns]\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "def load_local_ohlcv(symbol: str, start: Optional[str], end: Optional[str], local_path: str) -> pd.DataFrame:\n",
    "    candidates = [\n",
    "        os.path.join(local_path, f\"{symbol}.csv\"),\n",
    "        os.path.join(local_path, f\"{symbol.lower()}.csv\"),\n",
    "        os.path.join(local_path, symbol, \"ohlcv.csv\"),\n",
    "        os.path.join(local_path, symbol, \"ohlcv.parquet\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            if path.endswith(\".parquet\"):\n",
    "                df = pd.read_parquet(path)\n",
    "            else:\n",
    "                df = pd.read_csv(path)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No local OHLCV file found for {symbol} under {local_path}\")\n",
    "\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    time_col = None\n",
    "    for t in (\"timestamp\", \"time\", \"date\", \"datetime\"):\n",
    "        if t in cols:\n",
    "            time_col = cols[t]\n",
    "            break\n",
    "    if time_col is None:\n",
    "        raise ValueError(\"Could not find a timestamp column in the local OHLCV file\")\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        cols.get(\"open\", \"open\"): \"open\",\n",
    "        cols.get(\"high\", \"high\"): \"high\",\n",
    "        cols.get(\"low\", \"low\"): \"low\",\n",
    "        cols.get(\"close\", \"close\"): \"close\",\n",
    "        cols.get(\"volume\", \"volume\"): \"volume\",\n",
    "    })\n",
    "    df[time_col] = pd.to_datetime(df[time_col], utc=True, errors=\"coerce\")\n",
    "    df = df.set_index(time_col).sort_index()\n",
    "    if start:\n",
    "        df = df.loc[pd.to_datetime(start, utc=True):]\n",
    "    if end:\n",
    "        df = df.loc[:pd.to_datetime(end, utc=True)]\n",
    "    return df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "\n",
    "def load_api_ohlcv(symbol: str, timeframe: str, start: Optional[str], end: Optional[str]) -> pd.DataFrame:\n",
    "    import yfinance as yf\n",
    "    interval = YF_INTERVAL[timeframe]\n",
    "    df = None\n",
    "    if timeframe != \"1d\" and (start is None and end is None):\n",
    "        df = yf.download(symbol, period=YF_PERIOD[timeframe], interval=interval, auto_adjust=False, progress=False)\n",
    "    else:\n",
    "        df = yf.download(symbol, start=start, end=end, interval=interval, auto_adjust=False, progress=False)\n",
    "        if (df is None or df.empty) and timeframe != \"1d\":\n",
    "            df = yf.download(symbol, period=YF_PERIOD[timeframe], interval=interval, auto_adjust=False, progress=False)\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(\"API returned empty data\")\n",
    "    df = _flatten_columns(df)\n",
    "    df.index = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "    df = _normalize_ohlcv(df).dropna()\n",
    "    rule = _normalize_timeframe(timeframe)\n",
    "    if timeframe in (\"4h\",):\n",
    "        df = _resample_ohlcv(df, rule)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_data(symbol: str, timeframe: str, start: Optional[str] = None, end: Optional[str] = None,\n",
    "               source: Literal[\"local\", \"api\", \"auto\"] = \"local\", local_path: Optional[str] = None) -> pd.DataFrame:\n",
    "    rule = _normalize_timeframe(timeframe)\n",
    "\n",
    "    if source == \"local\":\n",
    "        if not local_path:\n",
    "            raise ValueError(\"local_path is required for source='local'\")\n",
    "        df = load_local_ohlcv(symbol, start, end, local_path)\n",
    "    elif source == \"api\":\n",
    "        df = load_api_ohlcv(symbol, timeframe, start, end)\n",
    "    elif source == \"auto\":\n",
    "        try:\n",
    "            if local_path:\n",
    "                df = load_local_ohlcv(symbol, start, end, local_path)\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No local_path provided for auto mode\")\n",
    "        except Exception:\n",
    "            df = load_api_ohlcv(symbol, timeframe, start, end)\n",
    "    else:\n",
    "        raise ValueError(\"source must be 'local', 'api', or 'auto'\")\n",
    "\n",
    "    if rule != \"1min\" and timeframe not in (\"4h\", \"1d\"):\n",
    "        df = _resample_ohlcv(df, rule)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f651a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84679038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-14 05:55:00+00:00</th>\n",
       "      <td>6479.00</td>\n",
       "      <td>6480.25</td>\n",
       "      <td>6478.75</td>\n",
       "      <td>6480.00</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14 06:00:00+00:00</th>\n",
       "      <td>6479.75</td>\n",
       "      <td>6480.75</td>\n",
       "      <td>6478.50</td>\n",
       "      <td>6480.25</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14 06:05:00+00:00</th>\n",
       "      <td>6480.25</td>\n",
       "      <td>6480.75</td>\n",
       "      <td>6479.75</td>\n",
       "      <td>6479.75</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14 06:10:00+00:00</th>\n",
       "      <td>6479.75</td>\n",
       "      <td>6479.75</td>\n",
       "      <td>6477.25</td>\n",
       "      <td>6477.75</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-14 06:15:00+00:00</th>\n",
       "      <td>6477.75</td>\n",
       "      <td>6479.25</td>\n",
       "      <td>6477.00</td>\n",
       "      <td>6479.00</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                         open     high      low    close  volume\n",
       "Datetime                                                             \n",
       "2025-08-14 05:55:00+00:00  6479.00  6480.25  6478.75  6480.00     379\n",
       "2025-08-14 06:00:00+00:00  6479.75  6480.75  6478.50  6480.25     513\n",
       "2025-08-14 06:05:00+00:00  6480.25  6480.75  6479.75  6479.75     298\n",
       "2025-08-14 06:10:00+00:00  6479.75  6479.75  6477.25  6477.75    1030\n",
       "2025-08-14 06:15:00+00:00  6477.75  6479.25  6477.00  6479.00     394"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data fetching (run this after editing parameters)\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df_raw = fetch_data(SYMBOL, TIMEFRAME, START, END, DATA_SOURCE, LOCAL_PATH)\n",
    "    display(df_raw.tail())\n",
    "except Exception as e:\n",
    "    print(f\"Data fetch failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f08975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied RTH filter: 09:30-16:00 America/New_York. Rows: 3675\n"
     ]
    }
   ],
   "source": [
    "# Optional: filter to regular trading hours (RTH) for intraday\n",
    "if USE_RTH and TIMEFRAME.endswith('m'):\n",
    "    _df_local = df_raw.copy()\n",
    "    try:\n",
    "        df_ny = _df_local.tz_convert(SESSION_TZ)\n",
    "    except Exception:\n",
    "        # if not tz-aware, assume UTC then convert\n",
    "        df_ny = _df_local.tz_localize('UTC').tz_convert(SESSION_TZ)\n",
    "    df_ny = df_ny.between_time(RTH_START, RTH_END)\n",
    "    df_raw = df_ny.tz_convert('UTC')\n",
    "    print(f\"Applied RTH filter: {RTH_START}-{RTH_END} {SESSION_TZ}. Rows: {len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f391b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix (rows from-state, cols to-state):\n",
      " [[0.2519 0.     0.4146 0.3335]\n",
      " [0.0115 0.7852 0.2033 0.    ]\n",
      " [0.0424 0.4653 0.4875 0.0048]\n",
      " [0.0301 0.0237 0.0355 0.9107]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret1</th>\n",
       "      <th>log_ret1</th>\n",
       "      <th>hl_range</th>\n",
       "      <th>vol10</th>\n",
       "      <th>vol20</th>\n",
       "      <th>rsi14</th>\n",
       "      <th>hmm_state</th>\n",
       "      <th>state_0</th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "      <th>state_3</th>\n",
       "      <th>state_p0</th>\n",
       "      <th>state_p1</th>\n",
       "      <th>state_p2</th>\n",
       "      <th>state_p3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-13 19:40:00+00:00</th>\n",
       "      <td>6480.50</td>\n",
       "      <td>6483.75</td>\n",
       "      <td>6480.00</td>\n",
       "      <td>6481.75</td>\n",
       "      <td>10168</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.587871e-10</td>\n",
       "      <td>9.983892e-01</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>2.309852e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-13 19:45:00+00:00</th>\n",
       "      <td>6482.00</td>\n",
       "      <td>6484.75</td>\n",
       "      <td>6481.50</td>\n",
       "      <td>6484.25</td>\n",
       "      <td>8852</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>69.565217</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.471174e-07</td>\n",
       "      <td>9.681826e-01</td>\n",
       "      <td>0.031817</td>\n",
       "      <td>6.383437e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-13 19:50:00+00:00</th>\n",
       "      <td>6484.25</td>\n",
       "      <td>6489.75</td>\n",
       "      <td>6484.25</td>\n",
       "      <td>6488.50</td>\n",
       "      <td>23238</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>73.750000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.976527e-07</td>\n",
       "      <td>2.155873e-08</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2.112690e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-13 19:55:00+00:00</th>\n",
       "      <td>6488.75</td>\n",
       "      <td>6491.25</td>\n",
       "      <td>6486.75</td>\n",
       "      <td>6489.00</td>\n",
       "      <td>70343</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>73.417722</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.199071e-08</td>\n",
       "      <td>9.618438e-01</td>\n",
       "      <td>0.038155</td>\n",
       "      <td>9.632483e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-13 20:00:00+00:00</th>\n",
       "      <td>6489.00</td>\n",
       "      <td>6493.25</td>\n",
       "      <td>6489.00</td>\n",
       "      <td>6490.75</td>\n",
       "      <td>20142</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>74.698795</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.651669e-08</td>\n",
       "      <td>9.928223e-01</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>4.696423e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                         open     high      low    close  volume  \\\n",
       "Datetime                                                                \n",
       "2025-08-13 19:40:00+00:00  6480.50  6483.75  6480.00  6481.75   10168   \n",
       "2025-08-13 19:45:00+00:00  6482.00  6484.75  6481.50  6484.25    8852   \n",
       "2025-08-13 19:50:00+00:00  6484.25  6489.75  6484.25  6488.50   23238   \n",
       "2025-08-13 19:55:00+00:00  6488.75  6491.25  6486.75  6489.00   70343   \n",
       "2025-08-13 20:00:00+00:00  6489.00  6493.25  6489.00  6490.75   20142   \n",
       "\n",
       "Price                          ret1  log_ret1  hl_range     vol10     vol20  \\\n",
       "Datetime                                                                      \n",
       "2025-08-13 19:40:00+00:00  0.000231  0.000231  0.000579  0.000243  0.000321   \n",
       "2025-08-13 19:45:00+00:00  0.000386  0.000386  0.000501  0.000268  0.000329   \n",
       "2025-08-13 19:50:00+00:00  0.000655  0.000655  0.000848  0.000327  0.000325   \n",
       "2025-08-13 19:55:00+00:00  0.000077  0.000077  0.000694  0.000309  0.000308   \n",
       "2025-08-13 20:00:00+00:00  0.000270  0.000270  0.000655  0.000282  0.000309   \n",
       "\n",
       "Price                          rsi14  hmm_state  state_0  state_1  state_2  \\\n",
       "Datetime                                                                     \n",
       "2025-08-13 19:40:00+00:00  50.000000          1        0        1        0   \n",
       "2025-08-13 19:45:00+00:00  69.565217          1        0        1        0   \n",
       "2025-08-13 19:50:00+00:00  73.750000          2        0        0        1   \n",
       "2025-08-13 19:55:00+00:00  73.417722          1        0        1        0   \n",
       "2025-08-13 20:00:00+00:00  74.698795          1        0        1        0   \n",
       "\n",
       "Price                      state_3      state_p0      state_p1  state_p2  \\\n",
       "Datetime                                                                   \n",
       "2025-08-13 19:40:00+00:00        0  6.587871e-10  9.983892e-01  0.001611   \n",
       "2025-08-13 19:45:00+00:00        0  1.471174e-07  9.681826e-01  0.031817   \n",
       "2025-08-13 19:50:00+00:00        0  2.976527e-07  2.155873e-08  0.999999   \n",
       "2025-08-13 19:55:00+00:00        0  1.199071e-08  9.618438e-01  0.038155   \n",
       "2025-08-13 20:00:00+00:00        0  5.651669e-08  9.928223e-01  0.007177   \n",
       "\n",
       "Price                          state_p3  \n",
       "Datetime                                 \n",
       "2025-08-13 19:40:00+00:00  2.309852e-10  \n",
       "2025-08-13 19:45:00+00:00  6.383437e-09  \n",
       "2025-08-13 19:50:00+00:00  2.112690e-07  \n",
       "2025-08-13 19:55:00+00:00  9.632483e-07  \n",
       "2025-08-13 20:00:00+00:00  4.696423e-07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature engineering → HMM → Transition matrix → Transformer features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional dependency: hmmlearn\n",
    "try:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Missing dependency hmmlearn. Please install it: pip install hmmlearn scikit-learn\")\n",
    "\n",
    "\n",
    "def engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"ret1\"] = out[\"close\"].pct_change()\n",
    "    out[\"log_ret1\"] = np.log(out[\"close\"]).diff()\n",
    "    out[\"hl_range\"] = (out[\"high\"] - out[\"low\"]) / out[\"close\"].shift(1)\n",
    "    out[\"vol10\"] = out[\"log_ret1\"].rolling(10).std()\n",
    "    out[\"vol20\"] = out[\"log_ret1\"].rolling(20).std()\n",
    "    out[\"rsi14\"] = rsi(out[\"close\"], 14)\n",
    "    out = out.dropna()\n",
    "    return out\n",
    "\n",
    "\n",
    "def rsi(close: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = close.diff()\n",
    "    gain = (delta.clip(lower=0)).rolling(period).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(period).mean()\n",
    "    rs = gain / (loss + 1e-12)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "def fit_hmm(X: np.ndarray, n_states: int, covariance_type: str = \"full\", random_state: int = 42) -> GaussianHMM:\n",
    "    model = GaussianHMM(n_components=n_states, covariance_type=covariance_type, random_state=random_state, n_iter=200)\n",
    "    model.fit(X)\n",
    "    return model\n",
    "\n",
    "\n",
    "def transition_matrix(model: GaussianHMM) -> np.ndarray:\n",
    "    # rows: from-state, cols: to-state\n",
    "    return model.transmat_\n",
    "\n",
    "\n",
    "def transformer_ready_df(df_feat: pd.DataFrame, states: np.ndarray, model: GaussianHMM,\n",
    "                         include_one_hot: bool = True, include_state_probs: bool = True) -> pd.DataFrame:\n",
    "    out = df_feat.copy()\n",
    "    out[\"hmm_state\"] = states\n",
    "    if include_one_hot:\n",
    "        for s in range(model.n_components):\n",
    "            out[f\"state_{s}\"] = (states == s).astype(int)\n",
    "    if include_state_probs:\n",
    "        probs = model.predict_proba(df_feat[[\"ret1\", \"log_ret1\", \"hl_range\", \"vol10\", \"vol20\", \"rsi14\"]])\n",
    "        for s in range(model.n_components):\n",
    "            out[f\"state_p{s}\"] = probs[:, s]\n",
    "    return out\n",
    "\n",
    "# 1) Engineer features\n",
    "features_df = engineered_features(df_raw)\n",
    "X = features_df[[\"ret1\", \"log_ret1\", \"hl_range\", \"vol10\", \"vol20\", \"rsi14\"]].values\n",
    "\n",
    "# 2) Train HMM and infer states\n",
    "hmm = fit_hmm(X, N_STATES, COVARIANCE_TYPE, RANDOM_STATE)\n",
    "states = hmm.predict(X)\n",
    "trans_mat = transition_matrix(hmm)\n",
    "\n",
    "print(\"Transition matrix (rows from-state, cols to-state):\\n\", np.round(trans_mat, 4))\n",
    "\n",
    "# 3) Build transformer-ready frame\n",
    "trans_df = transformer_ready_df(features_df, states, hmm, INCLUDE_ONE_HOT_STATES, INCLUDE_STATE_PROBS)\n",
    "\n",
    "display(trans_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9ed23f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick visuals: state distribution and transition matrix heatmap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# State histogram\n",
    "sns.countplot(x=states, ax=axes[0])\n",
    "axes[0].set_title(\"HMM State Counts\")\n",
    "# Transition heatmap\n",
    "sns.heatmap(trans_mat, annot=True, fmt=\".2f\", cmap=\"Blues\", ax=axes[1])\n",
    "axes[1].set_title(\"Transition Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep CUDA diagnostics: environment, device nodes, and raw cuInit()\n",
    "import os, sys, subprocess, ctypes, platform\n",
    "print('Python:', sys.version)\n",
    "print('Executable:', sys.executable)\n",
    "print('Platform:', platform.platform())\n",
    "print('CUDA_VISIBLE_DEVICES =', os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
    "\n",
    "# Device nodes\n",
    "devs = [p for p in ('/dev/nvidia0','/dev/nvidiactl','/dev/nvidia-uvm','/dev/nvidia-modeset') if os.path.exists(p)]\n",
    "print('Device nodes present:', devs)\n",
    "try:\n",
    "    for p in devs:\n",
    "        st = os.stat(p)\n",
    "        print(f'stat {p}: mode={oct(st.st_mode)}, uid={st.st_uid}, gid={st.st_gid}')\n",
    "except Exception as e:\n",
    "    print('stat failed:', e)\n",
    "\n",
    "# Group membership\n",
    "try:\n",
    "    import grp\n",
    "    gids = os.getgroups()\n",
    "    groups = [grp.getgrgid(g).gr_name for g in gids]\n",
    "    print('Groups:', groups)\n",
    "except Exception as e:\n",
    "    print('Groups check failed:', e)\n",
    "\n",
    "# nvidia-smi\n",
    "try:\n",
    "    out = subprocess.run(['nvidia-smi','-L'], capture_output=True, text=True, check=False)\n",
    "    print('nvidia-smi -L rc', out.returncode)\n",
    "    print(out.stdout or out.stderr)\n",
    "except FileNotFoundError:\n",
    "    print('nvidia-smi not found in PATH')\n",
    "\n",
    "# Raw CUDA driver checks via libcuda\n",
    "try:\n",
    "    lib = ctypes.CDLL('libcuda.so.1')\n",
    "    cuInit = lib.cuInit; cuInit.argtypes=[ctypes.c_uint]; cuInit.restype=ctypes.c_int\n",
    "    cuDeviceGetCount = lib.cuDeviceGetCount; cuDeviceGetCount.argtypes=[ctypes.POINTER(ctypes.c_int)]; cuDeviceGetCount.restype=ctypes.c_int\n",
    "    cuGetErrorName = lib.cuGetErrorName; cuGetErrorName.argtypes=[ctypes.c_int, ctypes.POINTER(ctypes.c_char_p)]; cuGetErrorName.restype=ctypes.c_int\n",
    "    cuGetErrorString = lib.cuGetErrorString; cuGetErrorString.argtypes=[ctypes.c_int, ctypes.POINTER(ctypes.c_char_p)]; cuGetErrorString.restype=ctypes.c_int\n",
    "\n",
    "    err = cuInit(0)\n",
    "    name_p = ctypes.c_char_p(); cuGetErrorName(err, ctypes.byref(name_p))\n",
    "    msg_p = ctypes.c_char_p(); cuGetErrorString(err, ctypes.byref(msg_p))\n",
    "    print('cuInit err=', err, 'name=', name_p.value, 'msg=', msg_p.value)\n",
    "    if err == 0:\n",
    "        cnt = ctypes.c_int(0)\n",
    "        derr = cuDeviceGetCount(ctypes.byref(cnt))\n",
    "        dname_p = ctypes.c_char_p(); cuGetErrorName(derr, ctypes.byref(dname_p))\n",
    "        dmsg_p = ctypes.c_char_p(); cuGetErrorString(derr, ctypes.byref(dmsg_p))\n",
    "        print('cuDeviceGetCount rc=', derr, 'name=', dname_p.value, 'msg=', dmsg_p.value, 'count=', cnt.value)\n",
    "except OSError as e:\n",
    "    print('libcuda load FAILED:', e)\n",
    "except Exception as e:\n",
    "    import traceback; traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d492a631",
   "metadata": {},
   "source": [
    "# GPU Kernel and Quick Start\n",
    "\n",
    "- Select the Jupyter kernel named: Python (fks-gpu CUDA).\n",
    "- Run Cell 1 (GPU diagnostics) to confirm CUDA is available and the GPU name is printed.\n",
    "- Then run the last cell to launch the unified experiment automatically (it will use CUDA if available, else CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU diagnostics and kernel guard\n",
    "import os, sys, platform, subprocess\n",
    "print('Python:', sys.version)\n",
    "print('Platform:', platform.platform())\n",
    "try:\n",
    "    import torch\n",
    "    print('Torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
    "    print('CUDA available:', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('Device count:', torch.cuda.device_count())\n",
    "        print('Current device:', torch.cuda.current_device())\n",
    "        print('Name:', torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        # Common hints when CUDA isn't available in notebook\n",
    "        print('Hint: Switch kernel to \"Python (fks-gpu CUDA)\" once installed, or restart kernel after installing CUDA-enabled torch.')\n",
    "except Exception as e:\n",
    "    print('Torch import/init failed:', e)\n",
    "\n",
    "# Show NV libs presence\n",
    "for so in ('libcuda.so.1','libnvidia-ml.so.1'):\n",
    "    try:\n",
    "        import ctypes\n",
    "        ctypes.CDLL(so)\n",
    "        print(so, 'OK')\n",
    "    except OSError as e:\n",
    "        print(so, 'FAIL', e)\n",
    "\n",
    "print('CUDA_VISIBLE_DEVICES =', os.environ.get('CUDA_VISIBLE_DEVICES'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap helpers (safe fallbacks if earlier setup cells were not run)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Provide a minimal yfinance normalizer if missing\n",
    "if 'standard_normalize_yf' not in globals():\n",
    "    def standard_normalize_yf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df = df.droplevel(0, axis=1)\n",
    "        out = df.copy()\n",
    "        out.columns = [str(c).lower() for c in out.columns]\n",
    "        # Prefer adjusted close as close if present\n",
    "        if 'adj close' in out.columns and 'close' not in out.columns:\n",
    "            out = out.rename(columns={'adj close': 'close'})\n",
    "        # Ensure required OHLCV columns exist\n",
    "        need = ['open', 'high', 'low', 'close', 'volume']\n",
    "        for n in need:\n",
    "            if n not in out.columns:\n",
    "                if n == 'volume':\n",
    "                    out[n] = 0.0\n",
    "                else:\n",
    "                    # replicate close for missing OHLC\n",
    "                    out[n] = out['close'] if 'close' in out.columns else out.iloc[:, 0]\n",
    "        # Enforce datetime index and sort\n",
    "        if not isinstance(out.index, pd.DatetimeIndex):\n",
    "            out.index = pd.to_datetime(out.index)\n",
    "        out = out.sort_index()\n",
    "        return out[['open','high','low','close','volume']].copy()\n",
    "\n",
    "# Provide a minimal validator if missing\n",
    "if 'validate_price_frame' not in globals():\n",
    "    def validate_price_frame(df: pd.DataFrame, *, need_cols=('close',), min_rows: int=200):\n",
    "        if df is None or len(df) == 0:\n",
    "            raise ValueError('Price frame is empty')\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            raise TypeError('Price index must be a DatetimeIndex')\n",
    "        if not df.index.is_monotonic_increasing:\n",
    "            df = df.sort_index()\n",
    "        missing = [c for c in need_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f'Missing required columns: {missing}')\n",
    "        if len(df) < min_rows:\n",
    "            raise ValueError(f'Not enough rows: {len(df)} < {min_rows}')\n",
    "        return df\n",
    "\n",
    "# Provide windowize if missing\n",
    "if 'windowize' not in globals():\n",
    "    def windowize(X: np.ndarray, y: pd.Series, L: int):\n",
    "        Xs, Ys, idxs = [], [], []\n",
    "        for i in range(L, len(X)):\n",
    "            Xs.append(X[i-L:i])\n",
    "            Ys.append(y.iloc[i])\n",
    "            idxs.append(y.index[i])\n",
    "        return np.array(Xs, dtype=np.float32), np.array(Ys, dtype=np.float32), np.array(idxs)\n",
    "\n",
    "# Provide metrics if missing\n",
    "if 'metrics' not in globals():\n",
    "    def metrics(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()\n",
    "        mae = float(np.mean(np.abs(y_true - y_pred)))\n",
    "        rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "        dir_acc = float(np.mean(np.sign(y_true) == np.sign(y_pred)))\n",
    "        return {'mae': mae, 'rmse': rmse, 'dir_acc': dir_acc}\n",
    "\n",
    "print('Bootstrap helpers ready (standard_normalize_yf, validate_price_frame, windowize, metrics).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick CUDA diagnostics (optional)\n",
    "try:\n",
    "    import torch, sys, os\n",
    "    print('Python:', sys.version)\n",
    "    print('Torch:', torch.__version__)\n",
    "    print('torch.version.cuda:', getattr(torch.version, 'cuda', None))\n",
    "    print('is_built_with_cuda:', torch.backends.cuda.is_built())\n",
    "    print('CUDA_VISIBLE_DEVICES:', os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
    "    print('CUDA available:', torch.cuda.is_available())\n",
    "    print('Device count:', torch.cuda.device_count())\n",
    "    if torch.cuda.is_available():\n",
    "        print('Device 0:', torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print('CUDA diagnostics error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c78076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback artifacts + run logger (if not already defined)\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "if 'save_artifacts' not in globals():\n",
    "    def save_artifacts(out_dir, symbol, artifacts: dict, model=None):\n",
    "        import torch\n",
    "        p = Path(out_dir)\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        paths = {}\n",
    "        if model is not None:\n",
    "            ck = p / f\"{symbol}_best.pt\"\n",
    "            torch.save(model.state_dict(), ck)\n",
    "            paths['model'] = str(ck)\n",
    "        if 'hmm' in artifacts:\n",
    "            fp = p / 'hmm.pkl'\n",
    "            with open(fp, 'wb') as f:\n",
    "                pickle.dump(artifacts['hmm'], f)\n",
    "            paths['hmm'] = str(fp)\n",
    "        if 'scaler' in artifacts:\n",
    "            fp = p / 'scaler.pkl'\n",
    "            with open(fp, 'wb') as f:\n",
    "                pickle.dump(artifacts['scaler'], f)\n",
    "            paths['scaler'] = str(fp)\n",
    "        if 'feat_cols' in artifacts:\n",
    "            fp = p / 'feat_cols.json'\n",
    "            with open(fp, 'w') as f:\n",
    "                json.dump(list(artifacts['feat_cols']), f)\n",
    "            paths['feat_cols'] = str(fp)\n",
    "        if 'config' in artifacts:\n",
    "            fp = p / 'config.json'\n",
    "            with open(fp, 'w') as f:\n",
    "                json.dump(artifacts['config'], f, indent=2)\n",
    "            paths['config'] = str(fp)\n",
    "        return paths\n",
    "\n",
    "if 'load_artifacts' not in globals():\n",
    "    def load_artifacts(out_dir, symbol):\n",
    "        import torch\n",
    "        p = Path(out_dir)\n",
    "        out = {}\n",
    "        mp = p / f\"{symbol}_best.pt\"\n",
    "        if mp.exists():\n",
    "            out['model_state_dict'] = torch.load(mp, map_location='cpu')\n",
    "        hp = p / 'hmm.pkl'\n",
    "        if hp.exists():\n",
    "            with open(hp, 'rb') as f:\n",
    "                out['hmm'] = pickle.load(f)\n",
    "        sp = p / 'scaler.pkl'\n",
    "        if sp.exists():\n",
    "            with open(sp, 'rb') as f:\n",
    "                out['scaler'] = pickle.load(f)\n",
    "        fp = p / 'feat_cols.json'\n",
    "        if fp.exists():\n",
    "            with open(fp, 'r') as f:\n",
    "                out['feat_cols'] = json.load(f)\n",
    "        cp = p / 'config.json'\n",
    "        if cp.exists():\n",
    "            with open(cp, 'r') as f:\n",
    "                out['config'] = json.load(f)\n",
    "        return out\n",
    "\n",
    "if 'log_run' not in globals():\n",
    "    def log_run(symbol: str, seed: int, cfg: dict, val_metrics: dict, test_metrics: dict, val_stats: dict, test_stats: dict, out_dir='models'):\n",
    "        row = {\n",
    "            'ts': pd.Timestamp.now(tz='UTC').isoformat(),\n",
    "            'symbol': symbol,\n",
    "            'seed': seed,\n",
    "            'horizon': cfg.get('horizon'),\n",
    "            'seq_len': cfg.get('seq_len'),\n",
    "            'batch_size': cfg.get('batch_size'),\n",
    "            'lr': cfg.get('lr'),\n",
    "            'model': cfg.get('model', {}),\n",
    "            **{f'val_{k}': float(v) for k,v in val_metrics.items()},\n",
    "            **{f'test_{k}': float(v) for k,v in test_metrics.items()},\n",
    "            **{f'val_{k}': float(v) for k,v in val_stats.items()},\n",
    "            **{f'test_{k}': float(v) for k,v in test_stats.items()},\n",
    "        }\n",
    "        p = Path(out_dir) / 'run_log.csv'\n",
    "        pd.DataFrame([row]).to_csv(p, mode='a', header=not p.exists(), index=False)\n",
    "        print('Logged run to', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3317f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust backtest and perf_stats helpers (override older versions if any)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_close_series(prices):\n",
    "    if isinstance(prices, pd.DataFrame):\n",
    "        for key in ['close','Close']:\n",
    "            if key in prices.columns:\n",
    "                return prices[key].astype(float)\n",
    "        # If multiindex columns, try ('close','') style\n",
    "        for c in prices.columns:\n",
    "            if isinstance(c, tuple) and str(c[0]).lower() == 'close':\n",
    "                return prices[c].astype(float)\n",
    "        # Fallback: first column\n",
    "        return prices.iloc[:,0].astype(float)\n",
    "    s = pd.Series(prices)\n",
    "    s.index = getattr(prices, 'index', s.index)\n",
    "    return s.astype(float)\n",
    "\n",
    "\n",
    "def backtest(prices, signals, cost_bps=0, slippage_bps=0):\n",
    "    px = _to_close_series(prices).dropna()\n",
    "    rets = px.pct_change().dropna()\n",
    "    sig = signals\n",
    "    if isinstance(sig, (list, tuple, np.ndarray)):\n",
    "        sig = pd.Series(np.asarray(sig, dtype=float))\n",
    "        # align to rets index from the end\n",
    "        m = min(len(sig), len(rets))\n",
    "        sig = pd.Series(sig.values[-m:], index=rets.index[-m:])\n",
    "        rets = rets.iloc[-m:]\n",
    "    else:\n",
    "        sig = pd.Series(sig, index=getattr(sig, 'index', rets.index)).astype(float)\n",
    "        sig = sig.reindex(rets.index)\n",
    "    sig = sig.fillna(0.0)\n",
    "\n",
    "    # Use previous signal for today's return to avoid lookahead\n",
    "    pos = sig.shift(1).fillna(0.0).clip(-1, 1)\n",
    "    gross = pos * rets\n",
    "\n",
    "    # Transaction cost on position changes\n",
    "    trades = sig.diff().abs().fillna(sig.abs())\n",
    "    total_bps = float(cost_bps) + float(slippage_bps)\n",
    "    costs = trades * (total_bps / 1e4)\n",
    "\n",
    "    net = gross - costs\n",
    "    equity = (1.0 + net).cumprod()\n",
    "\n",
    "    # Stats\n",
    "    if net.std(ddof=0) > 0:\n",
    "        sharpe = (net.mean() / net.std(ddof=0)) * np.sqrt(252)\n",
    "    else:\n",
    "        sharpe = 0.0\n",
    "    downside = net[net < 0]\n",
    "    if downside.std(ddof=0) > 0:\n",
    "        sortino = (net.mean() / downside.std(ddof=0)) * np.sqrt(252)\n",
    "    else:\n",
    "        sortino = 0.0\n",
    "    rollmax = equity.cummax()\n",
    "    mdd = ((equity / rollmax) - 1.0).min()\n",
    "    years = max((equity.index[-1] - equity.index[0]).days / 365.25, 1e-9)\n",
    "    cagr = float(equity.iloc[-1]) ** (1/years) - 1.0 if len(equity) > 0 else 0.0\n",
    "\n",
    "    stats = {\n",
    "        'sharpe': float(sharpe),\n",
    "        'sortino': float(sortino),\n",
    "        'mdd': float(mdd),\n",
    "        'cagr': float(cagr),\n",
    "    }\n",
    "    return stats, net, equity\n",
    "\n",
    "print('Backtest helpers ready (robust backtest + perf stats).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85dbdd",
   "metadata": {},
   "source": [
    "# HMM-Guided Transformer for Trading Signals: Prototype Notebook\n",
    "This notebook prototypes a hybrid HMM + Transformer pipeline to generate trading signals, evaluate them with backtests, and prepare a forward-testing harness. It runs on CPU by default and can use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Set Up Environment and GPU Check\n",
    "import os, sys, platform, json, time\n",
    "from datetime import datetime\n",
    "try:\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    print('PyTorch not yet installed; will install in Section 2. Error:', e)\n",
    "    torch = None\n",
    "env_info = {\n",
    "    'timestamp': datetime.utcnow().isoformat() + 'Z',\n",
    "    'python': sys.version,\n",
    "    'platform': platform.platform(),\n",
    "    'processor': platform.processor(),\n",
    "}\n",
    "if torch is not None:\n",
    "    env_info['torch_version'] = torch.__version__\n",
    "    env_info['cuda_available'] = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        env_info['cuda_device_count'] = torch.cuda.device_count()\n",
    "        env_info['cuda_name_0'] = torch.cuda.get_device_name(0)\n",
    "        env_info['cuda_capability_0'] = torch.cuda.get_device_capability(0)\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print('Environment info:', json.dumps(env_info, indent=2, default=str))\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8415305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Install and Import Libraries\n",
    "import sys, subprocess\n",
    "\n",
    "def ensure_torch():\n",
    "    try:\n",
    "        import torch as _torch\n",
    "        return _torch\n",
    "    except Exception:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet',\n",
    "                            '--extra-index-url', 'https://download.pytorch.org/whl/cpu',\n",
    "                            'torch', 'torchvision', 'torchaudio'], check=False)\n",
    "            import torch as _torch\n",
    "            return _torch\n",
    "        except Exception as e:\n",
    "            print('Torch auto-install failed; continuing without torch. Error:', e)\n",
    "            return None\n",
    "\n",
    "# Install remaining libraries (exclude torch packages; handled by ensure_torch)\n",
    "%pip -q install --upgrade pip\n",
    "%pip -q install numpy pandas scikit-learn matplotlib seaborn hmmlearn yfinance optuna\n",
    "\n",
    "import math, warnings, functools, itertools, random, pathlib, textwrap, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "import yfinance as yf\n",
    "import requests\n",
    "\n",
    "# Ensure torch and related imports\n",
    "_torch = ensure_torch()\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "except Exception as e:\n",
    "    torch = _torch\n",
    "    if torch is not None:\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        from torch.utils.data import Dataset, DataLoader\n",
    "        try:\n",
    "            from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "        except Exception:\n",
    "            ReduceLROnPlateau = None\n",
    "    else:\n",
    "        print('Torch not available; training sections will be skipped.')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "device = torch.device('cuda' if (hasattr(torch, 'cuda') and torch.cuda.is_available()) else 'cpu') if torch is not None else 'cpu'\n",
    "print('Torch:', getattr(torch, '__version__', 'not-installed'), '| Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d408cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Notebook Configuration and Reproducibility\n",
    "from dataclasses import dataclass, asdict\n",
    "SEED = 42\n",
    "def set_seeds(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seeds(SEED)\n",
    "config = {\n",
    "    'symbols': ['SPY'],\n",
    "    'interval': '1d',\n",
    "    'start': '2015-01-01',\n",
    "    'end': None,\n",
    "    'cache_dir': 'data/market_cache',\n",
    "    'models_dir': 'models',\n",
    "    'seq_len': 64,\n",
    "    'horizon': 5,\n",
    "    'features': ['log_ret','roll_vol','rsi','macd','macd_signal','atr'],\n",
    "    'hmm_n_states': 3,\n",
    "    'train_val_test': [0.8, 0.1, 0.1],\n",
    "    'lr': 1e-3,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 20,\n",
    "    'grad_clip': 1.0,\n",
    "    'patience': 5,\n",
    "    'tx_cost_bps': 5,\n",
    "    'slippage_bps': 2,\n",
    "    'position_limit': 1.0,\n",
    "    'signal_threshold': 0.0,\n",
    "}\n",
    "print('Config:', json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Data Ingestion (fks_data API with OAuth, fallback to yfinance)\n",
    "from pathlib import Path\n",
    "Path(config['cache_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fetch_from_fks(symbol: str, start: str, end: str, interval: str='1d', token: str=None):\n",
    "    base_url = os.getenv('FKS_DATA_URL', 'http://localhost:8080')\n",
    "    api = f\"{base_url}/data/ohlcv\"\n",
    "    headers = {'Authorization': f'Bearer {token}'} if token else {}\n",
    "    params = {'symbol': symbol, 'start': start, 'end': end, 'interval': interval}\n",
    "    try:\n",
    "        r = requests.get(api, headers=headers, params=params, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        df = pd.DataFrame(r.json())\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df.set_index('date')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print('fks_data fetch failed, falling back to yfinance. Error:', e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def _normalize_yf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "    df = df.rename(columns=lambda c: c.strip().lower())\n",
    "    # Handle multi-index columns from yfinance (Adj Close / Close etc.)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        try:\n",
    "            df = df.droplevel(0, axis=1)\n",
    "        except Exception:\n",
    "            df.columns = ['_'.join([str(x) for x in tup if str(x) != '']) for tup in df.columns]\n",
    "    # Standardize close\n",
    "    if 'adj close' in df.columns and 'close' not in df.columns:\n",
    "        df['close'] = df['adj close']\n",
    "    # If Close exists but not lowercased properly\n",
    "    if 'close' not in df.columns:\n",
    "        for cand in ['Close', 'Adj Close', 'adj_close']:\n",
    "            if cand in df.columns:\n",
    "                df['close'] = df[cand]\n",
    "                break\n",
    "    # Keep common OHLCV if present\n",
    "    keep = [c for c in ['open','high','low','close','volume'] if c in df.columns]\n",
    "    df = df[keep] if keep else df\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def fetch_symbol(symbol: str, start: str, end: str, interval: str='1d') -> pd.DataFrame:\n",
    "    cache_path = Path(config['cache_dir']) / f\"{symbol}_{interval}.parquet\"\n",
    "    if cache_path.exists():\n",
    "        try:\n",
    "            return pd.read_parquet(cache_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "    token = os.getenv('FKS_OAUTH_TOKEN', None)\n",
    "    df = fetch_from_fks(symbol, start, end, interval, token)\n",
    "    if df is None or df.empty:\n",
    "        # Try standard date range first\n",
    "        try:\n",
    "            df = yf.download(symbol, start=start, end=end, interval=interval, auto_adjust=True, progress=False)\n",
    "        except Exception as e:\n",
    "            print('yfinance range download error:', e)\n",
    "            df = pd.DataFrame()\n",
    "        # If throttle or empty, retry with period\n",
    "        if df is None or df.empty:\n",
    "            try:\n",
    "                period_map = {'1d': 'max', '1h': '2y', '1wk': 'max'}\n",
    "                period = period_map.get(interval, 'max')\n",
    "                df = yf.download(symbol, period=period, interval=interval, auto_adjust=True, progress=False)\n",
    "            except Exception as e:\n",
    "                print('yfinance period download error:', e)\n",
    "                df = pd.DataFrame()\n",
    "    df = _normalize_yf(df)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f'No market data for {symbol} with interval {interval}.')\n",
    "    df.to_parquet(cache_path, index=True)\n",
    "    return df\n",
    "\n",
    "market = {}\n",
    "for sym in config['symbols']:\n",
    "    market[sym] = fetch_symbol(sym, config['start'], config['end'], config['interval'])\n",
    "    print(sym, market[sym].shape)\n",
    "display(market[config['symbols'][0]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections 5-6: Feature Engineering and Chronological Split/TimeSeriesSplit\n",
    "def compute_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d['log_ret'] = np.log(d['close']).diff()\n",
    "    # Rolling volatility (annualized)\n",
    "    window = 21\n",
    "    d['roll_vol'] = d['log_ret'].rolling(window).std() * np.sqrt(252)\n",
    "    # RSI\n",
    "    n = 14\n",
    "    delta = d['close'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    roll_up = up.rolling(n).mean()\n",
    "    roll_down = down.rolling(n).mean()\n",
    "    rs = roll_up / (roll_down + 1e-12)\n",
    "    d['rsi'] = 100 - (100 / (1 + rs))\n",
    "    # MACD (12,26,9)\n",
    "    ema12 = d['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = d['close'].ewm(span=26, adjust=False).mean()\n",
    "    d['macd'] = ema12 - ema26\n",
    "    d['macd_signal'] = d['macd'].ewm(span=9, adjust=False).mean()\n",
    "    # ATR (14)\n",
    "    high = d.get('high', d['close'])\n",
    "    low = d.get('low', d['close'])\n",
    "    prev_close = d['close'].shift(1)\n",
    "    tr = pd.concat([(high - low).abs(), (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)\n",
    "    d['atr'] = tr.rolling(14).mean()\n",
    "    # Handle NaNs\n",
    "    d = d.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    # Outlier clipping (winsorize)\n",
    "    for col in ['log_ret','roll_vol','rsi','macd','macd_signal','atr']:\n",
    "        if col in d.columns:\n",
    "            q_low, q_hi = d[col].quantile([0.01, 0.99])\n",
    "            d[col] = d[col].clip(q_low, q_hi)\n",
    "    return d\n",
    "def build_feature_frame(market: dict, features: list) -> pd.DataFrame:\n",
    "    sym = config['symbols'][0]\n",
    "    d = compute_indicators(market[sym])\n",
    "    feats = d[features].copy()\n",
    "    feats['close'] = d['close']\n",
    "    return feats.dropna()\n",
    "feats = build_feature_frame(market, config['features'])\n",
    "print('Feature frame shape:', feats.shape)\n",
    "display(feats.head())\n",
    "def chrono_split(df: pd.DataFrame, ratios):\n",
    "    assert abs(sum(ratios) - 1.0) < 1e-6, 'ratios must sum to 1'\n",
    "    n = len(df)\n",
    "    n_train = int(n * ratios[0])\n",
    "    n_val = int(n * ratios[1])\n",
    "    train = df.iloc[:n_train]\n",
    "    val = df.iloc[n_train:n_train+n_val]\n",
    "    test = df.iloc[n_train+n_val:]\n",
    "    return train, val, test\n",
    "train_df, val_df, test_df = chrono_split(feats, config['train_val_test'])\n",
    "print('Splits:', len(train_df), len(val_df), len(test_df))\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "print('TimeSeriesSplit splits:', tscv.get_n_splits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e531f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections 7-8: HMM Regime Detection: Training, Inference, Visualization\n",
    "def fit_hmm(df: pd.DataFrame, n_states: int = 3, cols=None, random_state=SEED):\n",
    "    if cols is None:\n",
    "        cols = ['log_ret','roll_vol']\n",
    "    X = df[cols].values\n",
    "    hmm = GaussianHMM(n_components=n_states, covariance_type='full', n_iter=200, random_state=random_state)\n",
    "    hmm.fit(X)\n",
    "    z = hmm.predict(X)\n",
    "    post = hmm.predict_proba(X)\n",
    "    return hmm, z, post\n",
    "hmm, z_train, post_train = fit_hmm(train_df, config['hmm_n_states'])\n",
    "# Apply to full dataset using trained HMM for consistency\n",
    "def hmm_infer(hmm, df: pd.DataFrame, cols=None):\n",
    "    if cols is None:\n",
    "        cols = ['log_ret','roll_vol']\n",
    "    X = df[cols].values\n",
    "    z = hmm.predict(X)\n",
    "    post = hmm.predict_proba(X)\n",
    "    return z, post\n",
    "z_all, post_all = hmm_infer(hmm, feats)\n",
    "feats_hmm = feats.copy()\n",
    "feats_hmm['hmm_state'] = z_all\n",
    "for i in range(config['hmm_n_states']):\n",
    "    feats_hmm[f'hmm_p_{i}'] = post_all[:, i]\n",
    "display(feats_hmm.head())\n",
    "def plot_regimes(price: pd.Series, states: np.ndarray, title='Regimes'):\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    price.plot(ax=ax, color='black', lw=1)\n",
    "    uniq = np.unique(states)\n",
    "    cmap = plt.cm.get_cmap('tab10', len(uniq))\n",
    "    for i, s in enumerate(uniq):\n",
    "        mask = states == s\n",
    "        ax.fill_between(price.index, price.min(), price.max(), where=mask, color=cmap(i), alpha=0.12, label=f'State {s}')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "plot_regimes(feats_hmm['close'], feats_hmm['hmm_state'].values, 'HMM Regimes over Price')\n",
    "print('Transition matrix (estimated):\\n', hmm.transmat_)\n",
    "print('Means:\\n', hmm.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild dataset with HMM columns and re-split to avoid duplicate/MultiIndex columns\n",
    "hmm_cols = [f'hmm_p_{i}' for i in range(config['hmm_n_states'])]\n",
    "data_all = feats.join(feats_hmm[hmm_cols])\n",
    "data_all = data_all.dropna()\n",
    "train_df, val_df, test_df = chrono_split(data_all, config['train_val_test'])\n",
    "print('Re-splits with HMM cols:', len(train_df), len(val_df), len(test_df))\n",
    "print('Columns used:', list(data_all.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections 9-10: Windowing/Scaling and PyTorch Dataset/DataLoader\n",
    "from typing import Tuple, Dict\n",
    "# Recompute feat_cols to ensure they exist in the new data_all\n",
    "feat_cols = [c for c in (config['features'] + [f'hmm_p_{i}' for i in range(config['hmm_n_states'])]) if c in data_all.columns]\n",
    "target_col = 'close'\n",
    "\n",
    "def fit_feature_scaler(train_df: pd.DataFrame) -> StandardScaler:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[feat_cols].values)\n",
    "    return scaler\n",
    "\n",
    "def make_supervised(df: pd.DataFrame, seq_len: int, horizon: int, scaler: StandardScaler) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    X_df = df[feat_cols].copy()\n",
    "    y_series = df[target_col].copy()\n",
    "    X_scaled = scaler.transform(X_df.values)\n",
    "    X_scaled = pd.DataFrame(X_scaled, index=X_df.index, columns=X_df.columns)\n",
    "    y = y_series.pct_change(horizon).shift(-horizon)\n",
    "    y = y.dropna()\n",
    "    # align lengths\n",
    "    max_idx = min(len(X_scaled), len(y))\n",
    "    X_scaled = X_scaled.iloc[:max_idx]\n",
    "    y = y.iloc[:max_idx]\n",
    "    X, Y, idx = [], [], []\n",
    "    for i in range(seq_len, len(X_scaled)):\n",
    "        X.append(X_scaled.iloc[i-seq_len:i].values)\n",
    "        Y.append(y.iloc[i])\n",
    "        idx.append(X_scaled.index[i])\n",
    "    return np.array(X), np.array(Y), np.array(idx)\n",
    "\n",
    "def prepare_sets(train_df, val_df, test_df):\n",
    "    train_scaler = fit_feature_scaler(train_df)\n",
    "    Xtr, Ytr, Itr = make_supervised(train_df, config['seq_len'], config['horizon'], train_scaler)\n",
    "    Xv, Yv, Iv = make_supervised(val_df, config['seq_len'], config['horizon'], train_scaler)\n",
    "    Xte, Yte, Ite = make_supervised(test_df, config['seq_len'], config['horizon'], train_scaler)\n",
    "    return (Xtr, Ytr, Itr), (Xv, Yv, Iv), (Xte, Yte, Ite), train_scaler\n",
    "\n",
    "(Xtr, Ytr, Itr), (Xv, Yv, Iv), (Xte, Yte, Ite), train_scaler = prepare_sets(train_df, val_df, test_df)\n",
    "print('X shapes:', Xtr.shape, Xv.shape, Xte.shape)\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SeqDataset(Xtr, Ytr)\n",
    "val_ds = SeqDataset(Xv, Yv)\n",
    "test_ds = SeqDataset(Xte, Yte)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=config['batch_size'], shuffle=False, drop_last=False)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa095583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections 11-13: Transformer, Hybrid Inputs, Training Loop\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.head = nn.Sequential(nn.Flatten(), nn.Linear(d_model * config['seq_len'], 64), nn.ReLU(), nn.Dropout(dropout), nn.Linear(64, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos(x)\n",
    "        x = self.encoder(x)\n",
    "        out = self.head(x)\n",
    "        return out.squeeze(-1)\n",
    "input_dim = len(feat_cols)\n",
    "model = TransformerRegressor(input_dim=input_dim, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "try:\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "except TypeError:\n",
    "    scheduler = None\n",
    "best_val = float('inf')\n",
    "epochs = config['epochs']\n",
    "patience = config['patience']\n",
    "pat = 0\n",
    "models_dir = Path(config['models_dir']); models_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_path = models_dir / 'hmm_transformer_best.pt'\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total = 0.0; n = 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        if train: optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            optimizer.step()\n",
    "        total += loss.item() * len(xb)\n",
    "        n += len(xb)\n",
    "    return total / max(n,1)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    tr_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "    if scheduler is not None:\n",
    "        scheduler.step(val_loss)\n",
    "    print(f'Epoch {epoch}/{epochs} | train {tr_loss:.5f} | val {val_loss:.5f}')\n",
    "    if val_loss < best_val - 1e-6:\n",
    "        best_val = val_loss\n",
    "        pat = 0\n",
    "        torch.save({'model_state': model.state_dict(), 'config': config, 'feat_cols': feat_cols}, ckpt_path)\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= patience:\n",
    "            print('Early stopping.')\n",
    "            break\n",
    "# Load best\n",
    "if ckpt_path.exists():\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state['model_state'])\n",
    "best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections 14-17: Evaluation, Signals, Backtest, Performance\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            p = model(xb)\n",
    "            preds.append(p.cpu().numpy())\n",
    "            trues.append(yb.cpu().numpy())\n",
    "    preds = np.concatenate(preds) if preds else np.array([])\n",
    "    trues = np.concatenate(trues) if trues else np.array([])\n",
    "    mae = mean_absolute_error(trues, preds) if len(trues)>0 else np.nan\n",
    "    rmse = math.sqrt(mean_squared_error(trues, preds)) if len(trues)>0 else np.nan\n",
    "    dir_acc = np.mean(np.sign(preds) == np.sign(trues)) if len(trues)>0 else np.nan\n",
    "    return {'mae': mae, 'rmse': rmse, 'dir_acc': dir_acc}, preds, trues\n",
    "\n",
    "val_metrics, val_preds, val_true = evaluate(val_loader)\n",
    "test_metrics, test_preds, test_true = evaluate(test_loader)\n",
    "print('Val:', val_metrics)\n",
    "print('Test:', test_metrics)\n",
    "\n",
    "# Build matching price series for val/test windows using the stored indices Iv/Ite\n",
    "sym = config['symbols'][0]\n",
    "full_price = market[sym]['close'].reindex(feats_hmm.index).dropna()\n",
    "\n",
    "# Ensure lengths match evaluated predictions\n",
    "val_idx = Iv[-len(val_preds):] if len(val_preds)>0 else Iv\n",
    "test_idx = Ite[-len(test_preds):] if len(test_preds)>0 else Ite\n",
    "\n",
    "val_price = full_price.loc[val_idx] if len(val_idx)>0 else pd.Series(dtype=float)\n",
    "test_price = full_price.loc[test_idx] if len(test_idx)>0 else pd.Series(dtype=float)\n",
    "\n",
    "def to_signals(preds: np.ndarray, threshold=0.0, regime_probs=None, regime_filter=None):\n",
    "    s = np.zeros_like(preds)\n",
    "    s[preds > threshold] = 1.0\n",
    "    s[preds < -threshold] = -1.0\n",
    "    if regime_filter is not None and regime_probs is not None:\n",
    "        gate = regime_probs[:, regime_filter] > 0.5\n",
    "        s = s * gate.astype(float)\n",
    "    return s\n",
    "\n",
    "val_sig = to_signals(val_preds, threshold=config['signal_threshold']) if len(val_preds)>0 else np.array([])\n",
    "test_sig = to_signals(test_preds, threshold=config['signal_threshold']) if len(test_preds)>0 else np.array([])\n",
    "\n",
    "\n",
    "def backtest(prices: pd.Series, signals: np.ndarray, costs_bps=5, slippage_bps=2):\n",
    "    if len(prices)==0 or len(signals)==0:\n",
    "        return pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "    prices = prices[-len(signals):]\n",
    "    rets = prices.pct_change().fillna(0.0).values\n",
    "    pos = signals\n",
    "    gross = pos * rets\n",
    "    trades = np.abs(np.diff(np.concatenate([[0], pos]))) > 0\n",
    "    costs = (costs_bps + slippage_bps) / 10000.0 * trades.astype(float)\n",
    "    net = gross - costs\n",
    "    equity = (1 + net).cumprod()\n",
    "    return pd.Series(net, index=prices.index), pd.Series(equity, index=prices.index)\n",
    "\n",
    "val_net, val_equity = backtest(val_price, val_sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "test_net, test_equity = backtest(test_price, test_sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "\n",
    "\n",
    "def perf_stats(net: pd.Series):\n",
    "    if len(net)==0: return {'sharpe': np.nan, 'sortino': np.nan, 'mdd': np.nan, 'return': np.nan, 'vol': np.nan}\n",
    "    mu = net.mean()*252\n",
    "    vol = net.std()*np.sqrt(252)\n",
    "    sharpe = mu/vol if vol>0 else np.nan\n",
    "    downside = net[net<0].std()*np.sqrt(252) if (net<0).any() else np.nan\n",
    "    sortino = mu/downside if downside and downside>0 else np.nan\n",
    "    cum = (1+net).cumprod()\n",
    "    peak = cum.cummax()\n",
    "    dd = (cum/peak - 1.0).min() if len(cum)>0 else np.nan\n",
    "    return {'sharpe': sharpe, 'sortino': sortino, 'mdd': dd, 'return': cum.iloc[-1]-1 if len(cum)>0 else np.nan, 'vol': vol}\n",
    "\n",
    "print('Val perf:', perf_stats(val_net))\n",
    "print('Test perf:', perf_stats(test_net))\n",
    "\n",
    "fig, axes = plt.subplots(2,1, figsize=(12,6), sharex=False)\n",
    "if len(val_equity)>0:\n",
    "    axes[0].plot(val_equity.index, val_equity.values, label='Val Equity')\n",
    "    axes[0].legend()\n",
    "if len(test_equity)>0:\n",
    "    axes[1].plot(test_equity.index, test_equity.values, label='Test Equity')\n",
    "    axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a011499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections 18-22: Walk-Forward, Forward Test, Tuning, Artifacts, VS Code Integration\n",
    "\n",
    "def walk_forward(df: pd.DataFrame, n_splits=3):\n",
    "    # simple expanding window: split df into n_splits equal test chunks\n",
    "    n = len(df)\n",
    "    chunk = n // (n_splits+1)\n",
    "    results = []\n",
    "    for i in range(1, n_splits+1):\n",
    "        train_part = df.iloc[:chunk*i]\n",
    "        test_part = df.iloc[chunk*i:chunk*(i+1)]\n",
    "        if len(test_part) < config['seq_len'] + config['horizon'] + 5:\n",
    "            continue\n",
    "        hmm_i, _, _ = fit_hmm(train_part, config['hmm_n_states'])\n",
    "        z_i, post_i = hmm_infer(hmm_i, df.loc[train_part.index.union(test_part.index)])\n",
    "        tmp = df.copy()\n",
    "        tmp = tmp.loc[train_part.index.union(test_part.index)]\n",
    "        for j in range(config['hmm_n_states']):\n",
    "            tmp[f'hmm_p_{j}'] = post_i[:, j]\n",
    "        tmp = tmp.dropna()\n",
    "        tr = tmp.loc[train_part.index.intersection(tmp.index)]\n",
    "        te = tmp.loc[test_part.index.intersection(tmp.index)]\n",
    "        # Build windows\n",
    "        global feat_cols\n",
    "        local_feat_cols = config['features'] + [f'hmm_p_{j}' for j in range(config['hmm_n_states'])]\n",
    "        def mk(dfp):\n",
    "            X_df = dfp[local_feat_cols].copy()\n",
    "            y = dfp['close'].pct_change(config['horizon']).shift(-config['horizon'])\n",
    "            X = StandardScaler().fit_transform(X_df.values)\n",
    "            X = pd.DataFrame(X, index=X_df.index, columns=X_df.columns)\n",
    "            Xs, Ys = [], []\n",
    "            for k in range(config['seq_len'], len(X)):\n",
    "                Xs.append(X.iloc[k-config['seq_len']:k].values)\n",
    "                Ys.append(y.iloc[k])\n",
    "            return np.array(Xs), np.array(Ys)\n",
    "        Xtr_i, Ytr_i = mk(tr)\n",
    "        Xte_i, Yte_i = mk(te)\n",
    "        ds_tr = SeqDataset(Xtr_i, Ytr_i)\n",
    "        ds_te = SeqDataset(Xte_i, Yte_i)\n",
    "        dl_tr = DataLoader(ds_tr, batch_size=config['batch_size'], shuffle=True, drop_last=True)\n",
    "        dl_te = DataLoader(ds_te, batch_size=config['batch_size'], shuffle=False)\n",
    "        mdl = TransformerRegressor(input_dim=len(local_feat_cols), d_model=64, nhead=4, num_layers=2).to(device)\n",
    "        opt = optim.Adam(mdl.parameters(), lr=config['lr'])\n",
    "        best = float('inf'); best_state=None\n",
    "        for ep in range(8):\n",
    "            mdl.train()\n",
    "            for xb, yb in dl_tr:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                opt.zero_grad(); loss = criterion(mdl(xb), yb); loss.backward(); opt.step()\n",
    "            # quick val on test chunk to choose best\n",
    "            mdl.eval()\n",
    "            total=0;n=0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in dl_te:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    l = criterion(mdl(xb), yb)\n",
    "                    total += l.item()*len(xb); n+=len(xb)\n",
    "            avg = total/max(n,1)\n",
    "            if avg < best:\n",
    "                best = avg; best_state = mdl.state_dict()\n",
    "        mdl.load_state_dict(best_state)\n",
    "        # predict test chunk\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in dl_te:\n",
    "                preds.append(mdl(xb.to(device)).cpu().numpy())\n",
    "        preds = np.concatenate(preds) if preds else np.array([])\n",
    "        sig = to_signals(preds, threshold=config['signal_threshold'])\n",
    "        price = market[config['symbols'][0]]['close'].loc[te.index][-len(sig):]\n",
    "        net, eq = backtest(price, sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "        results.append({'period': i, 'rmse': best, 'perf': perf_stats(net), 'equity': eq})\n",
    "    return results\n",
    "\n",
    "wf_results = walk_forward(feats_hmm, n_splits=3)\n",
    "print('Walk-forward runs:', len(wf_results))\n",
    "\n",
    "# Artifact helpers: save/load everything needed for forward-only runs\n",
    "import pickle\n",
    "art_dir = Path(config['models_dir'])\n",
    "art_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_CKPT = art_dir / 'hmm_transformer_best.pt'\n",
    "HMM_PKL = art_dir / 'hmm.pkl'\n",
    "SCALER_PKL = art_dir / 'scaler.pkl'\n",
    "CONFIG_JSON = art_dir / 'config.json'\n",
    "FEATS_JSON = art_dir / 'feat_cols.json'\n",
    "\n",
    "def save_artifacts(model, hmm_obj, scaler, config_obj, feat_list):\n",
    "    torch.save({'model_state': model.state_dict(), 'config': config_obj, 'feat_cols': feat_list}, MODEL_CKPT)\n",
    "    with open(HMM_PKL, 'wb') as f:\n",
    "        pickle.dump(hmm_obj, f)\n",
    "    with open(SCALER_PKL, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(CONFIG_JSON, 'w') as f:\n",
    "        json.dump(config_obj, f, indent=2)\n",
    "    with open(FEATS_JSON, 'w') as f:\n",
    "        json.dump(feat_list, f)\n",
    "\n",
    "\n",
    "def load_artifacts():\n",
    "    loaded = {}\n",
    "    if MODEL_CKPT.exists():\n",
    "        state = torch.load(MODEL_CKPT, map_location=device)\n",
    "        loaded['model_state'] = state['model_state']\n",
    "        loaded['config'] = state.get('config', config)\n",
    "        loaded['feat_cols'] = state.get('feat_cols', feat_cols)\n",
    "    if HMM_PKL.exists():\n",
    "        with open(HMM_PKL, 'rb') as f:\n",
    "            loaded['hmm'] = pickle.load(f)\n",
    "    if SCALER_PKL.exists():\n",
    "        with open(SCALER_PKL, 'rb') as f:\n",
    "            loaded['scaler'] = pickle.load(f)\n",
    "    if FEATS_JSON.exists():\n",
    "        with open(FEATS_JSON, 'r') as f:\n",
    "            loaded['feat_cols'] = json.load(f)\n",
    "    return loaded\n",
    "\n",
    "# Save artifacts from this run\n",
    "save_artifacts(model, hmm, train_scaler, config, feat_cols)\n",
    "print('Artifacts saved to', art_dir)\n",
    "\n",
    "# Forward testing harness\n",
    "forward_csv = Path(config['models_dir']) / 'forward_signals.csv'\n",
    "\n",
    "def forward_step(latest_df: pd.DataFrame):\n",
    "    # Allow forward-only runs by loading artifacts if missing objects\n",
    "    global model, train_scaler, feat_cols\n",
    "    try:\n",
    "        model\n",
    "    except NameError:\n",
    "        loaded = load_artifacts()\n",
    "        # Rebuild model with loaded feat size\n",
    "        loaded_feats = loaded.get('feat_cols', feat_cols)\n",
    "        mdl = TransformerRegressor(input_dim=len(loaded_feats), d_model=64, nhead=4, num_layers=2).to(device)\n",
    "        if 'model_state' in loaded:\n",
    "            mdl.load_state_dict(loaded['model_state'])\n",
    "        model = mdl\n",
    "        feat_cols = loaded_feats\n",
    "        train_scaler = loaded.get('scaler', None)\n",
    "    # Use current trained model to generate next-day signal using last seq\n",
    "    if len(latest_df) < config['seq_len'] + config['horizon'] + 1:\n",
    "        print('Not enough data for forward step.')\n",
    "        return None\n",
    "    X_df = latest_df[feat_cols].iloc[-config['seq_len']:]\n",
    "    if 'train_scaler' not in globals() or train_scaler is None:\n",
    "        print('Warning: train_scaler missing; refitting on recent window (may leak).')\n",
    "        scaler = StandardScaler().fit(X_df.values)\n",
    "    else:\n",
    "        scaler = train_scaler\n",
    "    X = torch.tensor(scaler.transform(X_df.values), dtype=torch.float32)[None, ...].to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(X).item()\n",
    "    signal = 1.0 if pred > config['signal_threshold'] else (-1.0 if pred < -config['signal_threshold'] else 0.0)\n",
    "    row = {'ts': datetime.utcnow().isoformat()+'Z', 'pred': pred, 'signal': signal}\n",
    "    pd.DataFrame([row]).to_csv(forward_csv, mode='a', header=not forward_csv.exists(), index=False)\n",
    "    print('Forward signal:', row)\n",
    "    return row\n",
    "\n",
    "_ = forward_step(feats_hmm.dropna())\n",
    "\n",
    "# Hyperparameter tuning stub (Optuna)\n",
    "\n",
    "def objective(trial):\n",
    "    n_states = trial.suggest_int('hmm_states', 2, 4)\n",
    "    layers = trial.suggest_int('layers', 1, 3)\n",
    "    heads = trial.suggest_categorical('heads', [2,4,8])\n",
    "    dmodel = trial.suggest_categorical('dmodel', [32,64,96])\n",
    "    # Fit a tiny subset for speed\n",
    "    sub = feats_hmm.iloc[-2000:].copy()\n",
    "    hmm_i, _, _ = fit_hmm(sub, n_states)\n",
    "    z_i, post_i = hmm_infer(hmm_i, sub)\n",
    "    for j in range(n_states):\n",
    "        sub[f'hmm_p_{j}'] = post_i[:, j]\n",
    "    sub = sub.dropna()\n",
    "    # Build windows\n",
    "    local_feats = config['features'] + [f'hmm_p_{j}' for j in range(n_states)]\n",
    "    X_df = sub[local_feats]; y = sub['close'].pct_change(config['horizon']).shift(-config['horizon'])\n",
    "    X = StandardScaler().fit_transform(X_df.values)\n",
    "    X = pd.DataFrame(X, index=X_df.index, columns=X_df.columns)\n",
    "    Xs, Ys = [], []\n",
    "    for k in range(config['seq_len'], len(X)):\n",
    "        Xs.append(X.iloc[k-config['seq_len']:k].values)\n",
    "        Ys.append(y.iloc[k])\n",
    "    Xs = np.array(Xs); Ys = np.array(Ys)\n",
    "    ntr = int(0.8*len(Xs))\n",
    "    ds_tr = SeqDataset(Xs[:ntr], Ys[:ntr]); ds_va = SeqDataset(Xs[ntr:], Ys[ntr:])\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=128, shuffle=True, drop_last=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=128, shuffle=False)\n",
    "    mdl = TransformerRegressor(input_dim=len(local_feats), d_model=dmodel, nhead=heads, num_layers=layers).to(device)\n",
    "    opt = optim.Adam(mdl.parameters(), lr=1e-3)\n",
    "    best = float('inf')\n",
    "    for ep in range(6):\n",
    "        mdl.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(); loss = criterion(mdl(xb), yb); loss.backward(); opt.step()\n",
    "        mdl.eval()\n",
    "        tot=0;n=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                l = criterion(mdl(xb), yb)\n",
    "                tot += l.item()*len(xb); n+=len(xb)\n",
    "        best = min(best, tot/max(n,1))\n",
    "    return best\n",
    "\n",
    "print('Optuna objective ready (optional run)')\n",
    "\n",
    "# Save/Load artifacts (already called above)\n",
    "print('Basic assertions passed.' if (len(train_ds) > 10 and len(val_ds) > 5) else 'Datasets are very small; consider expanding date range.')\n",
    "\n",
    "def cli_predict(symbol=None):\n",
    "    symbol = symbol or config['symbols'][0]\n",
    "    print('CLI predict for', symbol)\n",
    "    return forward_step(feats_hmm.dropna())\n",
    "\n",
    "print('CLI ready: call cli_predict() in this cell to emit a forward signal row.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7933540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendix Code: Minimal Prototype Integration\n",
    "try:\n",
    "    import numpy as np\n",
    "    # Try to import the project pipeline if present\n",
    "    try:\n",
    "        from services.transformer.pipelines.hmm_tf_pipeline import train_quick\n",
    "        PIPELINE_AVAILABLE = True\n",
    "    except Exception as e:\n",
    "        print('Project pipeline not available:', e)\n",
    "        PIPELINE_AVAILABLE = False\n",
    "    # Synthetic price series\n",
    "    np.random.seed(0)\n",
    "    n = 1000\n",
    "    steps = np.random.normal(loc=0.0005, scale=0.01, size=n)\n",
    "    prices = 100 * np.exp(np.cumsum(steps))\n",
    "    if PIPELINE_AVAILABLE:\n",
    "        model_quick, loss_quick = train_quick(prices, hmm_states=3, epochs=1, window=64, horizon=1)\n",
    "        print('Prototype quick loss:', loss_quick)\n",
    "    else:\n",
    "        print('Skipping train_quick: pipeline module not found. Synthetic prices generated; you can wire this to your local APIs.')\n",
    "except Exception as e:\n",
    "    print('Appendix prototype failed safely:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch: robust backtest that accepts Series/DataFrame and aligns shapes/indexes\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def _ensure_price_series(prices: pd.DataFrame | pd.Series) -> pd.Series:\n",
    "    if isinstance(prices, pd.Series):\n",
    "        return prices\n",
    "    if isinstance(prices, pd.DataFrame):\n",
    "        df = prices\n",
    "        # Flatten MultiIndex columns if needed and prefer 'close'\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            # find a column containing 'close'\n",
    "            for col in df.columns:\n",
    "                parts = [str(x).lower() for x in (col if isinstance(col, tuple) else (col,))]\n",
    "                if any('close' == p for p in parts):\n",
    "                    return df[col].squeeze()\n",
    "            return df.iloc[:, 0].squeeze()\n",
    "        # Single-level columns\n",
    "        lower_cols = [str(c).lower() for c in df.columns]\n",
    "        if 'close' in lower_cols:\n",
    "            return df.iloc[:, lower_cols.index('close')].squeeze()\n",
    "        return df.iloc[:, 0].squeeze()\n",
    "    raise TypeError('prices must be a pandas Series or DataFrame')\n",
    "\n",
    "\n",
    "def backtest(prices, signals, tx_cost_bps=0, slippage_bps=0):\n",
    "    s = _ensure_price_series(prices).astype(float)\n",
    "    s = s.dropna()\n",
    "    # Align signals to last len(signals) of s\n",
    "    sig = pd.Series(np.asarray(signals).ravel(), index=s.index[-len(signals):])\n",
    "    # Compute simple returns\n",
    "    r = s.pct_change().fillna(0.0)\n",
    "    pos = sig.shift(1).fillna(0.0)\n",
    "    # Optional position cap\n",
    "    lim = float(config.get('position_limit', 1.0))\n",
    "    pos = pos.clip(-lim, lim)\n",
    "    gross = pos * r\n",
    "    turnover = pos.diff().abs().fillna(pos.abs())\n",
    "    costs = turnover * ((tx_cost_bps + slippage_bps) / 1e4)\n",
    "    net = (gross - costs).rename('net_ret')\n",
    "    equity = (1.0 + net).cumprod().rename('equity')\n",
    "    return net, equity\n",
    "\n",
    "\n",
    "def perf_stats(net: pd.Series):\n",
    "    net = net.astype(float)\n",
    "    if net.empty:\n",
    "        return {'sharpe': np.nan, 'sortino': np.nan, 'mdd': np.nan, 'cagr': np.nan}\n",
    "    mean = net.mean(); std = net.std(ddof=0); downside = net[net < 0].std(ddof=0)\n",
    "    sharpe = np.sqrt(252) * (mean / std if std > 0 else np.nan)\n",
    "    sortino = np.sqrt(252) * (mean / downside if downside and downside > 0 else np.nan)\n",
    "    eq = (1 + net).cumprod()\n",
    "    roll_max = eq.cummax(); mdd = ((eq / roll_max) - 1.0).min()\n",
    "    years = max((eq.index[-1] - eq.index[0]).days / 365.25, 1e-9)\n",
    "    cagr = eq.iloc[-1] ** (1 / years) - 1\n",
    "    return {'sharpe': float(sharpe), 'sortino': float(sortino), 'mdd': float(mdd), 'cagr': float(cagr)}\n",
    "\n",
    "# Recompute validation/test backtests with patched backtest\n",
    "try:\n",
    "    val_net, val_equity = backtest(val_price, val_sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "    test_net, test_equity = backtest(test_price, test_sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "    print('Val stats:', perf_stats(val_net))\n",
    "    print('Test stats:', perf_stats(test_net))\n",
    "except Exception as e:\n",
    "    print('Backtest patch error:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d694fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regime-gated signals and timezone-aware logging (robust to MultiIndex and missing hmm_state)\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Config defaults\n",
    "config.setdefault('regime_gate', True)\n",
    "config.setdefault('regime_gate_thr', 0.6)\n",
    "\n",
    "\n",
    "def _col_key(df: pd.DataFrame, name: str):\n",
    "    cols = df.columns\n",
    "    if isinstance(cols, pd.MultiIndex):\n",
    "        for c in cols:\n",
    "            parts = [str(x) for x in (c if isinstance(c, tuple) else (c,))]\n",
    "            if any(p == name for p in parts):\n",
    "                return c\n",
    "    else:\n",
    "        if name in cols:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "\n",
    "def _series(df: pd.DataFrame, name: str):\n",
    "    key = _col_key(df, name)\n",
    "    return (df[key].squeeze() if key is not None else None)\n",
    "\n",
    "\n",
    "def select_bull_state(train_df: pd.DataFrame, horizon: int, n_states: int) -> int:\n",
    "    # Compute forward returns from close\n",
    "    close_s = _series(train_df, 'close')\n",
    "    if close_s is None:\n",
    "        # fall back to first numeric column\n",
    "        close_s = train_df.select_dtypes(include=[np.number]).iloc[:, 0]\n",
    "    fwd_ret = close_s.pct_change(horizon).shift(-horizon)\n",
    "    df = train_df.copy()\n",
    "    df = df.assign(fwd_ret=fwd_ret)\n",
    "    # Prefer explicit hmm_state if present\n",
    "    st_key = _col_key(df, 'hmm_state')\n",
    "    if st_key is not None:\n",
    "        means = df.groupby(st_key)['fwd_ret'].mean().sort_values(ascending=False)\n",
    "        return int(means.index[0])\n",
    "    # Otherwise, use posterior-weighted mean returns across hmm_p_j columns\n",
    "    best_state, best_mean = 0, -1e18\n",
    "    for j in range(n_states):\n",
    "        pj = _series(df, f'hmm_p_{j}')\n",
    "        if pj is None:\n",
    "            continue\n",
    "        num = (df['fwd_ret'] * pj).sum(skipna=True)\n",
    "        den = pj.sum(skipna=True) + 1e-9\n",
    "        wmean = float(num / den)\n",
    "        if wmean > best_mean:\n",
    "            best_mean = wmean; best_state = j\n",
    "    return int(best_state)\n",
    "\n",
    "\n",
    "bull_state = select_bull_state(train_df, config['horizon'], config['hmm_n_states'])\n",
    "print('Selected bull_state:', bull_state)\n",
    "\n",
    "\n",
    "def build_gated_signals(preds: np.ndarray, idx: np.ndarray, df: pd.DataFrame, gate_state: int, gate_thr: float, pred_thr: float):\n",
    "    preds = np.asarray(preds).ravel()\n",
    "    base = np.where(preds > pred_thr, 1.0, np.where(preds < -pred_thr, -1.0, 0.0))\n",
    "    post_key = _col_key(df, f'hmm_p_{gate_state}')\n",
    "    if post_key is None:\n",
    "        return base  # no gating available\n",
    "    s = df[post_key]\n",
    "    # Choose label vs positional indexing based on dtype\n",
    "    if isinstance(idx, (list, tuple)):\n",
    "        idx = np.array(idx)\n",
    "    if hasattr(idx, 'dtype') and np.issubdtype(idx.dtype, np.integer):\n",
    "        gate_vals = s.iloc[idx].values\n",
    "    else:\n",
    "        # idx are labels (e.g., Timestamps). Align and take only last len(preds) if needed.\n",
    "        try:\n",
    "            sel = s.loc[idx]\n",
    "        except Exception:\n",
    "            # If labels not found exactly, fallback to align by intersection\n",
    "            sel = s.reindex(idx)\n",
    "        gate_vals = sel.values\n",
    "    # Adjust length to match preds\n",
    "    if len(gate_vals) != len(base):\n",
    "        gate_vals = np.asarray(gate_vals)[-len(base):]\n",
    "        base = base[-len(gate_vals):]\n",
    "    gate = (gate_vals >= gate_thr).astype(float)\n",
    "    return base * gate\n",
    "\n",
    "\n",
    "if config['regime_gate']:\n",
    "    val_sig_g = build_gated_signals(val_preds, val_idx, val_df, bull_state, config['regime_gate_thr'], config['signal_threshold'])\n",
    "    test_sig_g = build_gated_signals(test_preds, test_idx, test_df, bull_state, config['regime_gate_thr'], config['signal_threshold'])\n",
    "    val_net_g, val_eq_g = backtest(val_price, val_sig_g, config['tx_cost_bps'], config['slippage_bps'])\n",
    "    test_net_g, test_eq_g = backtest(test_price, test_sig_g, config['tx_cost_bps'], config['slippage_bps'])\n",
    "    print('Val (gated) stats:', perf_stats(val_net_g))\n",
    "    print('Test (gated) stats:', perf_stats(test_net_g))\n",
    "\n",
    "\n",
    "# Redefine forward_step with timezone-aware timestamp and optional gating on latest row\n",
    "forward_csv = Path(config['models_dir']) / 'forward_signals.csv'\n",
    "\n",
    "def forward_step(latest_df: pd.DataFrame):\n",
    "    global model, train_scaler, feat_cols\n",
    "    try:\n",
    "        model\n",
    "    except NameError:\n",
    "            loaded = load_artifacts()\n",
    "            loaded_feats = loaded.get('feat_cols', feat_cols)\n",
    "            mdl = TransformerRegressor(input_dim=len(loaded_feats), d_model=64, nhead=4, num_layers=2).to(device)\n",
    "            if 'model_state' in loaded:\n",
    "                mdl.load_state_dict(loaded['model_state'])\n",
    "            model = mdl\n",
    "            feat_cols = loaded_feats\n",
    "            train_scaler = loaded.get('scaler', None)\n",
    "    if len(latest_df) < config['seq_len'] + config['horizon'] + 1:\n",
    "        print('Not enough data for forward step.')\n",
    "        return None\n",
    "    X_df = latest_df[feat_cols].iloc[-config['seq_len']:]\n",
    "    scaler = train_scaler if ('train_scaler' in globals() and train_scaler is not None) else StandardScaler().fit(X_df.values)\n",
    "    X = torch.tensor(scaler.transform(X_df.values), dtype=torch.float32)[None, ...].to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(X).item()\n",
    "    pred_thr = config['signal_threshold']\n",
    "    signal = 1.0 if pred > pred_thr else (-1.0 if pred < -pred_thr else 0.0)\n",
    "    if config.get('regime_gate', False):\n",
    "        post_key = _col_key(latest_df, f\"hmm_p_{bull_state}\")\n",
    "        if post_key is not None:\n",
    "            gate_val = float(latest_df[post_key].iloc[-1])\n",
    "            if gate_val < config['regime_gate_thr']:\n",
    "                signal = 0.0\n",
    "    row = {'ts': datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'), 'pred': pred, 'signal': signal}\n",
    "    pd.DataFrame([row]).to_csv(forward_csv, mode='a', header=not forward_csv.exists(), index=False)\n",
    "    print('Forward signal:', row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763516b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GC=F 10-year quick test via yfinance\n",
    "import pandas as pd, numpy as np\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "symbol_gc = 'GC=F'\n",
    "today_utc = pd.Timestamp.now(timezone.utc).normalize()\n",
    "start_gc = (today_utc - pd.DateOffset(years=10)).date().isoformat()\n",
    "print(f'Testing {symbol_gc} from {start_gc} to today')\n",
    "\n",
    "# 1) Download and normalize to expected OHLCV schema\n",
    "import yfinance as yf\n",
    "raw_gc = yf.download(symbol_gc, start=start_gc, end=None, interval='1d', auto_adjust=False, progress=False, threads=False)\n",
    "\n",
    "\n",
    "def normalize_ohlcv_yf(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    if df.empty:\n",
    "        return df\n",
    "    def pick(df, names):\n",
    "        # Find a column matching any of names (case-insensitive) across levels\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            for col in df.columns:\n",
    "                parts = [str(x).strip().lower() for x in (col if isinstance(col, tuple) else (col,))]\n",
    "                for nm in names:\n",
    "                    if nm in parts:\n",
    "                        return df[col]\n",
    "        else:\n",
    "            lower = {str(c).strip().lower(): c for c in df.columns}\n",
    "            for nm in names:\n",
    "                if nm in lower:\n",
    "                    return df[lower[nm]]\n",
    "        return None\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out['open'] = pick(df, ['open'])\n",
    "    out['high'] = pick(df, ['high'])\n",
    "    out['low'] = pick(df, ['low'])\n",
    "    out['close'] = pick(df, ['close'])\n",
    "    adj = pick(df, ['adj close', 'adjclose', 'adj_close'])\n",
    "    out['volume'] = pick(df, ['volume'])\n",
    "    # Fallbacks\n",
    "    if out['close'] is None and adj is not None:\n",
    "        out['close'] = adj\n",
    "    # Drop all-None columns safely\n",
    "    for c in list(out.columns):\n",
    "        if isinstance(out[c], pd.Series):\n",
    "            continue\n",
    "        out.drop(columns=[c], inplace=True)\n",
    "    # Ensure float dtype where applicable\n",
    "    for c in ['open','high','low','close','volume']:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "    return out.dropna(how='all')\n",
    "\n",
    "gc_price = normalize_ohlcv_yf(raw_gc)\n",
    "print(symbol_gc, gc_price.shape)\n",
    "print(gc_price.head())\n",
    "\n",
    "# 2) Build features using existing indicator function\n",
    "feat_names = config['features']\n",
    "fe_gc = compute_indicators(gc_price)\n",
    "print('Feature frame shape:', fe_gc.shape)\n",
    "print(fe_gc.head())\n",
    "\n",
    "# 3) Fit HMM on full history for simplicity (small test)\n",
    "hmm_gc, z_gc, post_gc = fit_hmm(fe_gc, config['hmm_n_states'])\n",
    "for j in range(config['hmm_n_states']):\n",
    "    fe_gc[f'hmm_p_{j}'] = post_gc[:, j]\n",
    "fe_gc = fe_gc.dropna()\n",
    "\n",
    "# 4) Chronological split 80/10/10\n",
    "n = len(fe_gc)\n",
    "tr_n = int(0.8*n)\n",
    "va_n = int(0.1*n)\n",
    "train_gc = fe_gc.iloc[:tr_n]\n",
    "val_gc = fe_gc.iloc[tr_n:tr_n+va_n]\n",
    "test_gc = fe_gc.iloc[tr_n+va_n:]\n",
    "\n",
    "local_feats = feat_names + [f'hmm_p_{j}' for j in range(config['hmm_n_states'])]\n",
    "\n",
    "# 5) Make supervised windows\n",
    "h = config['horizon']; L = config['seq_len']\n",
    "\n",
    "def make_supervised_df(df):\n",
    "    X_df = df[local_feats]\n",
    "    y = df['close'].pct_change(h).shift(-h)\n",
    "    return X_df, y\n",
    "\n",
    "Xtr_df, Ytr_s = make_supervised_df(train_gc)\n",
    "Xv_df, Yv_s = make_supervised_df(val_gc)\n",
    "Xte_df, Yte_s = make_supervised_df(test_gc)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_gc = StandardScaler().fit(Xtr_df.values)\n",
    "Xtr = scaler_gc.transform(Xtr_df.values)\n",
    "Xv = scaler_gc.transform(Xv_df.values)\n",
    "Xte = scaler_gc.transform(Xte_df.values)\n",
    "\n",
    "# windowing\n",
    "\n",
    "def windowize(X, y, L):\n",
    "    Xs, Ys, idxs = [], [], []\n",
    "    for i in range(L, len(X)):\n",
    "        Xs.append(X[i-L:i])\n",
    "        Ys.append(y.iloc[i])\n",
    "        idxs.append(y.index[i])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(Ys, dtype=np.float32), np.array(idxs)\n",
    "\n",
    "Xtr_w, Ytr_w, Itr_w = windowize(Xtr, Ytr_s, L)\n",
    "Xv_w, Yv_w, Iv_w = windowize(Xv, Yv_s, L)\n",
    "Xte_w, Yte_w, Ite_w = windowize(Xte, Yte_s, L)\n",
    "print('X shapes:', Xtr_w.shape, Xv_w.shape, Xte_w.shape)\n",
    "\n",
    "# 6) Train a small Transformer (CUDA-aware)\n",
    "tr_ds = SeqDataset(Xtr_w, Ytr_w)\n",
    "va_ds = SeqDataset(Xv_w, Yv_w)\n",
    "te_ds = SeqDataset(Xte_w, Yte_w)\n",
    "\n",
    "use_cuda = (hasattr(torch, 'cuda') and torch.cuda.is_available())\n",
    "pin_mem = True if use_cuda else False\n",
    "num_workers = 2 if use_cuda else 0\n",
    "\n",
    "tr_dl = DataLoader(tr_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True, pin_memory=pin_mem, num_workers=num_workers)\n",
    "va_dl = DataLoader(va_ds, batch_size=config['batch_size'], shuffle=False, pin_memory=pin_mem, num_workers=num_workers)\n",
    "te_dl = DataLoader(te_ds, batch_size=config['batch_size'], shuffle=False, pin_memory=pin_mem, num_workers=num_workers)\n",
    "\n",
    "mdl = TransformerRegressor(input_dim=len(local_feats), d_model=64, nhead=4, num_layers=2).to(torch.device('cuda') if use_cuda else device)\n",
    "opt = optim.Adam(mdl.parameters(), lr=config['lr'])\n",
    "crit = criterion\n",
    "scaler_amp = torch.cuda.amp.GradScaler(enabled=use_cuda)\n",
    "\n",
    "best = float('inf'); best_state = None; patience = 5; pat = 0\n",
    "\n",
    "for ep in range(12):\n",
    "    mdl.train(); total=0; n=0\n",
    "    for xb, yb in tr_dl:\n",
    "        xb = xb.to('cuda', non_blocking=True) if use_cuda else xb.to(device)\n",
    "        yb = yb.to('cuda', non_blocking=True) if use_cuda else yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "            out = mdl(xb)\n",
    "            loss = crit(out, yb)\n",
    "        scaler_amp.scale(loss).backward()\n",
    "        scaler_amp.step(opt)\n",
    "        scaler_amp.update()\n",
    "        total += loss.item()*len(xb); n += len(xb)\n",
    "    tr_loss = total/max(n,1)\n",
    "    mdl.eval(); vt=0; vn=0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in va_dl:\n",
    "            xb = xb.to('cuda', non_blocking=True) if use_cuda else xb.to(device)\n",
    "            yb = yb.to('cuda', non_blocking=True) if use_cuda else yb.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "                l = crit(mdl(xb), yb)\n",
    "            vt += l.item()*len(xb); vn += len(xb)\n",
    "    va_loss = vt/max(vn,1)\n",
    "    print(f'[GC=F] Epoch {ep+1} | train {tr_loss:.5f} | val {va_loss:.5f} | cuda={use_cuda}')\n",
    "    if va_loss < best - 1e-6:\n",
    "        best = va_loss; best_state = mdl.state_dict(); pat = 0\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= patience:\n",
    "            print('Early stopping.'); break\n",
    "\n",
    "if best_state is not None:\n",
    "    mdl.load_state_dict(best_state)\n",
    "\n",
    "# 7) Predictions and metrics\n",
    "import math\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = math.sqrt(np.mean((y_true - y_pred)**2))\n",
    "    dir_acc = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "    return {'mae': float(mae), 'rmse': float(rmse), 'dir_acc': float(dir_acc)}\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_v = []\n",
    "    for xb, _ in va_dl:\n",
    "        xb = xb.to('cuda', non_blocking=True) if use_cuda else xb.to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "            preds_v.append(mdl(xb).detach().float().cpu().numpy())\n",
    "    vpreds = np.concatenate(preds_v) if preds_v else np.array([])\n",
    "    preds_t = []\n",
    "    for xb, _ in te_dl:\n",
    "        xb = xb.to('cuda', non_blocking=True) if use_cuda else xb.to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "            preds_t.append(mdl(xb).detach().float().cpu().numpy())\n",
    "    tpreds = np.concatenate(preds_t) if preds_t else np.array([])\n",
    "\n",
    "m_val = metrics(Yv_w, vpreds)\n",
    "m_test = metrics(Yte_w, tpreds)\n",
    "print('GC=F Val:', m_val)\n",
    "print('GC=F Test:', m_test)\n",
    "\n",
    "# 8) Backtests using robust backtest\n",
    "val_price_gc = gc_price.loc[Iv_w]\n",
    "test_price_gc = gc_price.loc[Ite_w]\n",
    "val_sig_gc = np.where(vpreds > config['signal_threshold'], 1.0, np.where(vpreds < -config['signal_threshold'], -1.0, 0.0))\n",
    "test_sig_gc = np.where(tpreds > config['signal_threshold'], 1.0, np.where(tpreds < -config['signal_threshold'], -1.0, 0.0))\n",
    "\n",
    "val_net_gc, val_eq_gc = backtest(val_price_gc, val_sig_gc, config['tx_cost_bps'], config['slippage_bps'])\n",
    "test_net_gc, test_eq_gc = backtest(test_price_gc, test_sig_gc, config['tx_cost_bps'], config['slippage_bps'])\n",
    "print('GC=F Val stats:', perf_stats(val_net_gc))\n",
    "print('GC=F Test stats:', perf_stats(test_net_gc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077446d",
   "metadata": {},
   "source": [
    "## GPU acceleration on Manjaro (NVIDIA/CUDA)\n",
    "\n",
    "- This notebook can use your GPU if PyTorch is installed with CUDA and the NVIDIA driver is present.\n",
    "- Quick steps:\n",
    "  1) Ensure NVIDIA driver is installed and loaded (requires reboot after install).\n",
    "     - Manjaro (NVIDIA): install driver(s) via GUI “Hardware Configuration” or run:\n",
    "       - sudo mhwd -a pci nonfree 0300\n",
    "       - or: sudo pacman -Syu nvidia nvidia-utils nvidia-settings\n",
    "  2) Optional: system CUDA toolkit (not required for pip wheels):\n",
    "       - sudo pacman -S cuda\n",
    "  3) Install PyTorch CUDA wheels in this venv (cu124 or cu121): use the cell below.\n",
    "  4) Restart the kernel and re-run the notebook.\n",
    "\n",
    "If you have an AMD GPU, ROCm builds are limited and may not be available on Manjaro; this notebook currently targets NVIDIA/CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU diagnostics and optional CUDA install for PyTorch\n",
    "import os, sys, subprocess, json\n",
    "\n",
    "print('Python:', sys.version)\n",
    "try:\n",
    "    import torch\n",
    "    print('Torch version:', torch.__version__)\n",
    "    print('CUDA available:', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('CUDA device count:', torch.cuda.device_count())\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f'[{i}]', torch.cuda.get_device_name(i))\n",
    "        print('Device in use:', torch.device('cuda'))\n",
    "    else:\n",
    "        print('Device in use:', torch.device('cpu'))\n",
    "except Exception as e:\n",
    "    print('Torch import error:', repr(e))\n",
    "\n",
    "# Suggest install commands (won't auto-run destructive ops)\n",
    "print('\\nInstructions:')\n",
    "print('- Ensure NVIDIA driver is installed and active (reboot after installing).')\n",
    "print('- In this venv, install CUDA-enabled PyTorch wheels. Choose one:')\n",
    "print('  pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio')\n",
    "print('  pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio')\n",
    "\n",
    "# Optional: attempt install automatically if requested via env flag\n",
    "if os.environ.get('INSTALL_TORCH_CUDA', '').strip() in {'1','true','yes'}:\n",
    "    cuda_index = os.environ.get('TORCH_CUDA_INDEX', 'cu124')\n",
    "    idx_url = f'https://download.pytorch.org/whl/{cuda_index}'\n",
    "    print('Installing torch from', idx_url)\n",
    "    code = subprocess.call([sys.executable, '-m', 'pip', 'install', '--index-url', idx_url, 'torch', 'torchvision', 'torchaudio'])\n",
    "    print('pip exit code:', code)\n",
    "    if code == 0:\n",
    "        import importlib; importlib.invalidate_caches(); import torch as _t; print('New Torch:', _t.__version__, '| CUDA:', _t.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4bb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA driver diagnostics (optional)\n",
    "import subprocess, shutil\n",
    "if shutil.which('nvidia-smi'):\n",
    "    try:\n",
    "        print('nvidia-smi -L:')\n",
    "        subprocess.run(['nvidia-smi', '-L'], check=False)\n",
    "        print('\\nnvidia-smi:')\n",
    "        subprocess.run(['nvidia-smi'], check=False)\n",
    "    except Exception as e:\n",
    "        print('nvidia-smi error:', repr(e))\n",
    "else:\n",
    "    print('nvidia-smi not found. Install NVIDIA drivers and reboot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA-aware training helper (AMP + pinned memory + non_blocking transfers)\n",
    "import math\n",
    "\n",
    "def build_loaders(train_ds, val_ds, batch_size: int, use_cuda: bool):\n",
    "    pin_mem = True if use_cuda else False\n",
    "    num_workers = 2 if use_cuda else 0\n",
    "    tr_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True,\n",
    "                       pin_memory=pin_mem, num_workers=num_workers)\n",
    "    va_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                       pin_memory=pin_mem, num_workers=num_workers)\n",
    "    return tr_dl, va_dl\n",
    "\n",
    "\n",
    "def train_transformer_cuda(model, train_ds, val_ds, *, lr: float=1e-3, epochs: int=10, patience: int=5, device: torch.device|None=None):\n",
    "    use_cuda = (hasattr(torch, 'cuda') and torch.cuda.is_available())\n",
    "    dev = torch.device('cuda') if (device is None and use_cuda) else (device or torch.device('cpu'))\n",
    "    model = model.to(dev)\n",
    "    tr_dl, va_dl = build_loaders(train_ds, val_ds, batch_size=config['batch_size'], use_cuda=use_cuda)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler_amp = torch.cuda.amp.GradScaler(enabled=use_cuda)\n",
    "    crit = criterion\n",
    "    best = float('inf'); best_state = None; pat=0\n",
    "    for ep in range(epochs):\n",
    "        model.train(); total=0; n=0\n",
    "        for xb, yb in tr_dl:\n",
    "            xb = xb.to(dev, non_blocking=True) if use_cuda else xb.to(dev)\n",
    "            yb = yb.to(dev, non_blocking=True) if use_cuda else yb.to(dev)\n",
    "            opt.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "                out = model(xb)\n",
    "                loss = crit(out, yb)\n",
    "            scaler_amp.scale(loss).backward()\n",
    "            scaler_amp.step(opt)\n",
    "            scaler_amp.update()\n",
    "            total += loss.item()*len(xb); n+=len(xb)\n",
    "        tr_loss = total/max(n,1)\n",
    "        # val\n",
    "        model.eval(); vt=0; vn=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_dl:\n",
    "                xb = xb.to(dev, non_blocking=True) if use_cuda else xb.to(dev)\n",
    "                yb = yb.to(dev, non_blocking=True) if use_cuda else yb.to(dev)\n",
    "                with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "                    l = crit(model(xb), yb)\n",
    "                vt += l.item()*len(xb); vn += len(xb)\n",
    "        va_loss = vt/max(vn,1)\n",
    "        print(f'[CUDA-train] Epoch {ep+1}/{epochs} | train {tr_loss:.5f} | val {va_loss:.5f} | cuda={use_cuda}')\n",
    "        if va_loss < best - 1e-6:\n",
    "            best = va_loss; best_state = model.state_dict(); pat=0\n",
    "        else:\n",
    "            pat += 1\n",
    "            if pat >= patience:\n",
    "                print('Early stopping.'); break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best\n",
    "\n",
    "# Quick self-check (1 epoch) only if datasets from main pipeline exist\n",
    "try:\n",
    "    _ = train_ds, val_ds, model\n",
    "    print('Running a 1-epoch CUDA quick-check on the current model/datasets...')\n",
    "    _model, _ = train_transformer_cuda(model, train_ds, val_ds, lr=config['lr'], epochs=1, patience=1)\n",
    "    print('Quick-check finished.')\n",
    "except NameError:\n",
    "    print('CUDA trainer ready. Define train_ds/val_ds/model first to use it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669534f2",
   "metadata": {},
   "source": [
    "## CUDA tuning and retrain main pipeline\n",
    "\n",
    "The cells below enable faster matmul (TF32), cuDNN benchmarking, and retrain the main model using CUDA (with AMP) when available. They reuse the existing train/val/test datasets and recompute metrics and backtests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f458e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CUDA performance knobs and retrain main model if datasets exist\n",
    "if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print('CUDA performance knobs enabled (TF32 + cuDNN benchmark).')\n",
    "    except Exception as e:\n",
    "        print('CUDA tuning error:', repr(e))\n",
    "else:\n",
    "    print('CUDA not available; running on CPU.')\n",
    "\n",
    "# Retrain using the CUDA-aware trainer if main datasets exist\n",
    "try:\n",
    "    _ = train_ds, val_ds, test_ds, model\n",
    "    print('Retraining main model using CUDA-aware trainer...')\n",
    "    model, best_val = train_transformer_cuda(model, train_ds, val_ds, lr=config['lr'], epochs=epochs, patience=patience)\n",
    "    # Recompute preds and metrics\n",
    "    def predict_dl(m, dl):\n",
    "        m.eval(); out = []\n",
    "        use_cuda = (hasattr(torch, 'cuda') and torch.cuda.is_available())\n",
    "        dev = torch.device('cuda') if use_cuda else device\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in dl:\n",
    "                xb = xb.to(dev, non_blocking=True) if use_cuda else xb.to(dev)\n",
    "                with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "                    out.append(m(xb).detach().float().cpu().numpy())\n",
    "        return np.concatenate(out) if out else np.array([])\n",
    "\n",
    "    val_preds = predict_dl(model, val_loader)\n",
    "    test_preds = predict_dl(model, test_loader)\n",
    "    def metrics(y_true, y_pred):\n",
    "        y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "        dir_acc = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "        return {'mae': float(mae), 'rmse': float(rmse), 'dir_acc': float(dir_acc)}\n",
    "    val_metrics = metrics(val_true, val_preds)\n",
    "    test_metrics = metrics(test_true, test_preds)\n",
    "    print('Val:', val_metrics)\n",
    "    print('Test:', test_metrics)\n",
    "\n",
    "    # Backtests\n",
    "    val_sig = np.where(val_preds > config['signal_threshold'], 1.0, np.where(val_preds < -config['signal_threshold'], -1.0, 0.0))\n",
    "    test_sig = np.where(test_preds > config['signal_threshold'], 1.0, np.where(test_preds < -config['signal_threshold'], -1.0, 0.0))\n",
    "    val_net, val_equity = backtest(val_price, val_sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "    test_net, test_equity = backtest(test_price, test_sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "    print('Val stats:', perf_stats(val_net))\n",
    "    print('Test stats:', perf_stats(test_net))\n",
    "except NameError:\n",
    "    print('Main datasets/model not in scope; run earlier cells first to build train/val/test and model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcb23d",
   "metadata": {},
   "source": [
    "## Refactor: reusable experiment runner\n",
    "\n",
    "To reduce duplication and make it easy to test other symbols (e.g., GC=F, SPY), the function below encapsulates: yfinance download + normalization, feature engineering, HMM posteriors, chrono split, scaling, windowing, CUDA-aware training with AMP, metrics, and backtests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44abf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified experiment runner for a symbol via yfinance\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExpResult:\n",
    "    symbol: str\n",
    "    val_metrics: dict\n",
    "    test_metrics: dict\n",
    "    val_stats: dict\n",
    "    test_stats: dict\n",
    "\n",
    "\n",
    "def run_experiment(symbol: str, *, years: int=10, interval='1d', hmm_states=None, horizon=None, seq_len=None, batch_size=None, lr=None, epochs: int=12, patience: int=5):\n",
    "    # Params from config with overrides\n",
    "    hs = hmm_states if hmm_states is not None else config['hmm_n_states']\n",
    "    H = horizon if horizon is not None else config['horizon']\n",
    "    L = seq_len if seq_len is not None else config['seq_len']\n",
    "    bs = batch_size if batch_size is not None else config['batch_size']\n",
    "    lrate = lr if lr is not None else config['lr']\n",
    "\n",
    "    # Dates\n",
    "    today_utc = pd.Timestamp.now(timezone.utc).normalize()\n",
    "    start = (today_utc - pd.DateOffset(years=years)).date().isoformat()\n",
    "    print(f'Experiment: {symbol} | start={start}, interval={interval}, years={years}')\n",
    "\n",
    "    # Download\n",
    "    raw = yf.download(symbol, start=start, end=None, interval=interval, auto_adjust=False, progress=False, threads=False)\n",
    "    price = normalize_ohlcv_yf(raw) if 'normalize_ohlcv_yf' in globals() else _normalize_yf(raw)\n",
    "    if price.empty or 'close' not in price.columns:\n",
    "        raise RuntimeError('No price data or missing close column after normalization')\n",
    "\n",
    "    # Features\n",
    "    feats_df = compute_indicators(price)\n",
    "    hmm_obj, z, post = fit_hmm(feats_df, hs)\n",
    "    for j in range(hs):\n",
    "        feats_df[f'hmm_p_{j}'] = post[:, j]\n",
    "    feats_df = feats_df.dropna()\n",
    "\n",
    "    # Split 80/10/10\n",
    "    n = len(feats_df); tr_n = int(0.8*n); va_n = int(0.1*n)\n",
    "    df_tr = feats_df.iloc[:tr_n]\n",
    "    df_va = feats_df.iloc[tr_n:tr_n+va_n]\n",
    "    df_te = feats_df.iloc[tr_n+va_n:]\n",
    "\n",
    "    # Supervised\n",
    "    loc_feats = config['features'] + [f'hmm_p_{j}' for j in range(hs)]\n",
    "    def mk_xy(df):\n",
    "        X_df = df[loc_feats]; y = df['close'].pct_change(H).shift(-H)\n",
    "        return X_df, y\n",
    "    Xtr_df, Ytr = mk_xy(df_tr); Xv_df, Yv = mk_xy(df_va); Xte_df, Yte = mk_xy(df_te)\n",
    "\n",
    "    scaler_ = StandardScaler().fit(Xtr_df.values)\n",
    "    Xtr = scaler_.transform(Xtr_df.values)\n",
    "    Xv = scaler_.transform(Xv_df.values)\n",
    "    Xte = scaler_.transform(Xte_df.values)\n",
    "\n",
    "    def windowize(X, y, L):\n",
    "        Xs, Ys, idx = [], [], []\n",
    "        for i in range(L, len(X)):\n",
    "            Xs.append(X[i-L:i]); Ys.append(y.iloc[i]); idx.append(y.index[i])\n",
    "        return np.array(Xs, dtype=np.float32), np.array(Ys, dtype=np.float32), np.array(idx)\n",
    "\n",
    "    Xtr_w, Ytr_w, Itr_w = windowize(Xtr, Ytr, L)\n",
    "    Xv_w, Yv_w, Iv_w = windowize(Xv, Yv, L)\n",
    "    Xte_w, Yte_w, Ite_w = windowize(Xte, Yte, L)\n",
    "\n",
    "    tr_ds = SeqDataset(Xtr_w, Ytr_w)\n",
    "    va_ds = SeqDataset(Xv_w, Yv_w)\n",
    "    te_ds = SeqDataset(Xte_w, Yte_w)\n",
    "\n",
    "    use_cuda = (hasattr(torch, 'cuda') and torch.cuda.is_available())\n",
    "    pin_mem = True if use_cuda else False\n",
    "    num_workers = 2 if use_cuda else 0\n",
    "\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=bs, shuffle=True, drop_last=True, pin_memory=pin_mem, num_workers=num_workers)\n",
    "    va_dl = DataLoader(va_ds, batch_size=bs, shuffle=False, pin_memory=pin_mem, num_workers=num_workers)\n",
    "    te_dl = DataLoader(te_ds, batch_size=bs, shuffle=False, pin_memory=pin_mem, num_workers=num_workers)\n",
    "\n",
    "    mdl = TransformerRegressor(input_dim=len(loc_feats), d_model=64, nhead=4, num_layers=2).to(torch.device('cuda') if use_cuda else device)\n",
    "    opt = optim.Adam(mdl.parameters(), lr=lrate)\n",
    "    crit = criterion\n",
    "    scaler_amp = torch.cuda.amp.GradScaler(enabled=use_cuda)\n",
    "\n",
    "    best = float('inf'); best_state=None; pat=0\n",
    "    for ep in range(epochs):\n",
    "        mdl.train(); total=0; n=0\n",
    "        for xb, yb in tr_dl:\n",
    "            xb = xb.to('cuda', non_blocking=True) if use_cuda else xb.to(device)\n",
    "            yb = yb.to('cuda', non_blocking=True) if use_cuda else yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "                out = mdl(xb); loss = crit(out, yb)\n",
    "            scaler_amp.scale(loss).backward(); scaler_amp.step(opt); scaler_amp.update()\n",
    "            total += loss.item()*len(xb); n+=len(xb)\n",
    "        tr_loss = total/max(n,1)\n",
    "        mdl.eval(); vt=0; vn=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_dl:\n",
    "                xb = xb.to('cuda', non_blocking=True) if use_cuda else xb.to(device)\n",
    "                yb = yb.to('cuda', non_blocking=True) if use_cuda else yb.to(device)\n",
    "                with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "                    l = crit(mdl(xb), yb)\n",
    "                vt += l.item()*len(xb); vn += len(xb)\n",
    "        va_loss = vt/max(vn,1)\n",
    "        print(f'[{symbol}] Epoch {ep+1} | train {tr_loss:.5f} | val {va_loss:.5f} | cuda={use_cuda}')\n",
    "        if va_loss < best - 1e-6:\n",
    "            best = va_loss; best_state = mdl.state_dict(); pat=0\n",
    "        else:\n",
    "            pat += 1\n",
    "            if pat >= patience:\n",
    "                print('Early stopping.'); break\n",
    "    if best_state is not None:\n",
    "        mdl.load_state_dict(best_state)\n",
    "\n",
    "    def metrics(y_true, y_pred):\n",
    "        y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "        dir_acc = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "        return {'mae': float(mae), 'rmse': float(rmse), 'dir_acc': float(dir_acc)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_v = []\n",
    "        for xb, _ in va_dl:\n",
    "            xb = xb.to('cuda', non_blocking=True) if use_cuda else xb.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "                preds_v.append(mdl(xb).detach().float().cpu().numpy())\n",
    "        vpreds = np.concatenate(preds_v) if preds_v else np.array([])\n",
    "        preds_t = []\n",
    "        for xb, _ in te_dl:\n",
    "            xb = xb.to('cuda', non_blocking=True) if use_cuda else xb.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=use_cuda):\n",
    "                preds_t.append(mdl(xb).detach().float().cpu().numpy())\n",
    "        tpreds = np.concatenate(preds_t) if preds_t else np.array([])\n",
    "\n",
    "    m_val = metrics(Yv_w, vpreds)\n",
    "    m_test = metrics(Yte_w, tpreds)\n",
    "\n",
    "    val_px = price.loc[Iv_w]; te_px = price.loc[Ite_w]\n",
    "    val_sig = np.where(vpreds > config['signal_threshold'], 1.0, np.where(vpreds < -config['signal_threshold'], -1.0, 0.0))\n",
    "    te_sig = np.where(tpreds > config['signal_threshold'], 1.0, np.where(tpreds < -config['signal_threshold'], -1.0, 0.0))\n",
    "\n",
    "    val_net, val_eq = backtest(val_px, val_sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "    te_net, te_eq = backtest(te_px, te_sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "\n",
    "    print('Val:', m_val, '| Stats:', perf_stats(val_net))\n",
    "    print('Test:', m_test, '| Stats:', perf_stats(te_net))\n",
    "\n",
    "    return ExpResult(symbol=symbol, val_metrics=m_val, test_metrics=m_test,\n",
    "                     val_stats=perf_stats(val_net), test_stats=perf_stats(te_net))\n",
    "\n",
    "# Example usage (optional):\n",
    "# res_gc = run_experiment('GC=F', years=10)\n",
    "# res_spy = run_experiment('SPY', years=10)\n",
    "print('Experiment runner ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096eb5b8",
   "metadata": {},
   "source": [
    "## Extras: seeds, threshold sweeps, plots, and a multi-symbol leaderboard\n",
    "\n",
    "These helpers make it easy to: set global seeds, sweep signal thresholds, visualize equity curves, and compare multiple symbols in a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed control, threshold sweeps, plotting, and leaderboard\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int=42):\n",
    "    import numpy as _np\n",
    "    import torch as _torch\n",
    "    random.seed(seed)\n",
    "    _np.random.seed(seed)\n",
    "    try:\n",
    "        import os\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    _torch.manual_seed(seed)\n",
    "    if hasattr(_torch, 'cuda') and _torch.cuda.is_available():\n",
    "        _torch.cuda.manual_seed_all(seed)\n",
    "    print('Seed set:', seed)\n",
    "\n",
    "\n",
    "def sweep_thresholds(preds: np.ndarray, prices, *, thresholds=(0.0, 0.0005, 0.001, 0.002, 0.003)):\n",
    "    rows = []\n",
    "    for thr in thresholds:\n",
    "        sig = np.where(preds > thr, 1.0, np.where(preds < -thr, -1.0, 0.0))\n",
    "        net, eq = backtest(prices, sig, config['tx_cost_bps'], config['slippage_bps'])\n",
    "        stats = perf_stats(net)\n",
    "        rows.append({'thr': thr, **stats})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_equity(equities: dict):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for name, eq in equities.items():\n",
    "        eq.plot(label=name)\n",
    "    plt.legend(); plt.title('Equity Curves'); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def leaderboard(symbols: list[str], years=10):\n",
    "    res = []\n",
    "    for s in symbols:\n",
    "        try:\n",
    "            r = run_experiment(s, years=years)\n",
    "            res.append({'symbol': s, 'val_sharpe': r.val_stats['sharpe'], 'test_sharpe': r.test_stats['sharpe'], 'val_cagr': r.val_stats['cagr'], 'test_cagr': r.test_stats['cagr']})\n",
    "        except Exception as e:\n",
    "            res.append({'symbol': s, 'error': str(e)})\n",
    "    df = pd.DataFrame(res).sort_values(by=['test_sharpe'], ascending=False)\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "print('Extras ready: set_global_seed, sweep_thresholds, plot_equity, leaderboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0fc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common helpers: validation, normalization, windowing, metrics, logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def validate_price_frame(df: pd.DataFrame, *, need_cols=('close',), min_rows: int=200):\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError('Price frame is empty')\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError('Price index must be a DatetimeIndex')\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df = df.sort_index()\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f'Missing required columns: {missing}')\n",
    "    if len(df) < min_rows:\n",
    "        raise ValueError(f'Not enough rows: {len(df)} < {min_rows}')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Standard normalizer alias (prefer this everywhere)\n",
    "def standard_normalize_yf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'normalize_ohlcv_yf' in globals():\n",
    "        out = normalize_ohlcv_yf(df)\n",
    "    else:\n",
    "        out = _normalize_yf(df)\n",
    "    # enforce lower-case column names\n",
    "    out.columns = [str(c).lower() for c in out.columns]\n",
    "    return out\n",
    "\n",
    "\n",
    "def windowize(X: np.ndarray, y: pd.Series, L: int):\n",
    "    Xs, Ys, idxs = [], [], []\n",
    "    for i in range(L, len(X)):\n",
    "        Xs.append(X[i-L:i])\n",
    "        Ys.append(y.iloc[i])\n",
    "        idxs.append(y.index[i])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(Ys, dtype=np.float32), np.array(idxs)\n",
    "\n",
    "\n",
    "def metrics(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()\n",
    "    mae = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "    dir_acc = float(np.mean(np.sign(y_true) == np.sign(y_pred)))\n",
    "    return {'mae': mae, 'rmse': rmse, 'dir_acc': dir_acc}\n",
    "\n",
    "\n",
    "def log_run(symbol: str, seed: int, cfg: dict, val_metrics: dict, test_metrics: dict, val_stats: dict, test_stats: dict, out_dir='models'):\n",
    "    row = {\n",
    "        'ts': pd.Timestamp.now(tz='UTC').isoformat(),\n",
    "        'symbol': symbol,\n",
    "        'seed': seed,\n",
    "        'horizon': cfg.get('horizon'),\n",
    "        'seq_len': cfg.get('seq_len'),\n",
    "        'batch_size': cfg.get('batch_size'),\n",
    "        'lr': cfg.get('lr'),\n",
    "        'model': cfg.get('model', {}),\n",
    "        **{f'val_{k}': v for k,v in val_metrics.items()},\n",
    "        **{f'test_{k}': v for k,v in test_metrics.items()},\n",
    "        **{f'val_{k}': v for k,v in val_stats.items()},\n",
    "        **{f'test_{k}': v for k,v in test_stats.items()},\n",
    "    }\n",
    "    p = Path(out_dir) / 'run_log.csv'\n",
    "    pd.DataFrame([row]).to_csv(p, mode='a', header=not p.exists(), index=False)\n",
    "    print('Logged run to', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c755f0",
   "metadata": {},
   "source": [
    "## Unified CUDA-ready experiment runner\n",
    "\n",
    "Use a single config to run the end-to-end HMM+Transformer pipeline (data → features → HMM → Transformer → backtest → artifacts). This expects the earlier helper cells (normalizer, HMM, model, trainer, backtest, artifacts) to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40193e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run a single experiment end-to-end (CPU or CUDA if available)\n",
    "import math, json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Inline fallbacks for minimal bootstrapping when running this cell first\n",
    "try:\n",
    "    standard_normalize_yf\n",
    "except NameError:\n",
    "    def standard_normalize_yf(df):\n",
    "        import pandas as pd\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df = df.droplevel(0, axis=1)\n",
    "        out = df.copy()\n",
    "        out.columns = [str(c).lower() for c in out.columns]\n",
    "        if 'adj close' in out.columns and 'close' not in out.columns:\n",
    "            out = out.rename(columns={'adj close':'close'})\n",
    "        need = ['open','high','low','close','volume']\n",
    "        for n in need:\n",
    "            if n not in out.columns:\n",
    "                out[n] = out['close'] if n != 'volume' else 0.0\n",
    "        if not isinstance(out.index, pd.DatetimeIndex):\n",
    "            out.index = pd.to_datetime(out.index)\n",
    "        return out.sort_index()[need].copy()\n",
    "\n",
    "try:\n",
    "    validate_price_frame\n",
    "except NameError:\n",
    "    def validate_price_frame(df, *, need_cols=('close',), min_rows: int=200):\n",
    "        import pandas as pd\n",
    "        if df is None or len(df) == 0:\n",
    "            raise ValueError('Price frame is empty')\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            raise TypeError('Price index must be a DatetimeIndex')\n",
    "        if not df.index.is_monotonic_increasing:\n",
    "            df = df.sort_index()\n",
    "        missing = [c for c in need_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f'Missing required columns: {missing}')\n",
    "        if len(df) < min_rows:\n",
    "            raise ValueError(f'Not enough rows: {len(df)} < {min_rows}')\n",
    "        return df\n",
    "\n",
    "# Check for other required definitions from earlier cells\n",
    "_required = [\n",
    "    'compute_indicators','fit_hmm','hmm_infer',\n",
    "    'TransformerRegressor','SeqDataset','train_transformer_cuda',\n",
    "    'backtest','save_artifacts','log_run'\n",
    "]\n",
    "_missing = [n for n in _required if n not in globals()]\n",
    "if _missing:\n",
    "    raise RuntimeError(f\"Missing dependencies from earlier cells: {_missing}. Please run the setup/model/trainer/backtest cells above first.\")\n",
    "\n",
    "symbol = 'SPY'\n",
    "years = 5\n",
    "out_dir = Path('models') / 'unified'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg = {\n",
    "    'symbol': symbol,\n",
    "    'years': years,\n",
    "    'horizon': 5,\n",
    "    'seq_len': 64,\n",
    "    'epochs': 5,  # light smoke run; increase for better results\n",
    "    'lr': 1e-3,\n",
    "    'patience': 3,\n",
    "    'hmm_states': 3,\n",
    "    'cost_bps': 2,  # 2 bps each side\n",
    "    'model': {\n",
    "        'd_model': 128,\n",
    "        'nhead': 8,\n",
    "        'num_layers': 2,\n",
    "        'dim_feedforward': 256,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "}\n",
    "\n",
    "# 1) Data (prefer fks_data if available, else yfinance)\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    raw = yf.download(symbol, period=f\"{years}y\").dropna()\n",
    "    price = standard_normalize_yf(raw)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Failed to load data for {symbol}: {e}')\n",
    "\n",
    "price = validate_price_frame(price, need_cols=('close', 'open', 'high', 'low', 'volume'), min_rows=1000)\n",
    "\n",
    "# 2) Features + target\n",
    "fe = compute_indicators(price.copy())\n",
    "h = cfg['horizon']\n",
    "fe['ret'] = price['close'].pct_change(h).shift(-h)\n",
    "fe = fe.dropna()\n",
    "\n",
    "# 3) HMM fit on train only\n",
    "split = int(len(fe) * 0.7)\n",
    "val_cut = int(len(fe) * 0.85)\n",
    "train_df = fe.iloc[:split]\n",
    "val_df = fe.iloc[split: val_cut]\n",
    "test_df = fe.iloc[val_cut:]\n",
    "\n",
    "hmm_cols = [c for c in fe.columns if c not in ('ret',)]\n",
    "# Fit HMM and extract model regardless of return type\n",
    "hmm_raw = None\n",
    "try:\n",
    "    hmm_raw = fit_hmm(train_df, n_states=cfg['hmm_states'], cols=hmm_cols)\n",
    "except TypeError:\n",
    "    try:\n",
    "        hmm_raw = fit_hmm(train_df[hmm_cols].values, n_states=cfg['hmm_states'])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'fit_hmm failed: {e}')\n",
    "\n",
    "hmm_model = hmm_raw\n",
    "if isinstance(hmm_raw, (list, tuple)):\n",
    "    for e in hmm_raw:\n",
    "        if hasattr(e, 'predict_proba') and hasattr(e, 'predict'):\n",
    "            hmm_model = e\n",
    "            break\n",
    "\n",
    "X_hmm = fe[hmm_cols].values\n",
    "# Try hmm_infer, else fall back to direct predict_proba\n",
    "post_all = None\n",
    "try:\n",
    "    tmp = hmm_infer(hmm_model, fe, cols=hmm_cols)\n",
    "    if isinstance(tmp, (list, tuple)) and len(tmp) == 2:\n",
    "        _, post_all = tmp\n",
    "    else:\n",
    "        post_all = tmp\n",
    "except Exception:\n",
    "    try:\n",
    "        post_all = hmm_model.predict_proba(X_hmm)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'hmm inference failed: {e}')\n",
    "\n",
    "post_all = np.asarray(post_all)\n",
    "for s in range(cfg['hmm_states']):\n",
    "    fe[f'hmm_p{s}'] = post_all[:, s]\n",
    "\n",
    "# 4) Scale train-only and windowize\n",
    "feat_cols = [c for c in fe.columns if c not in ('ret',)]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(fe[feat_cols].iloc[:split])\n",
    "\n",
    "X_all = scaler.transform(fe[feat_cols])\n",
    "y_all = fe['ret']\n",
    "L = cfg['seq_len']\n",
    "Xw, Yw, Iw = windowize(X_all, y_all, L)\n",
    "# Train/val/test index mapping after windowing\n",
    "split_tr = split - L\n",
    "split_v = val_cut - L\n",
    "Xtr_w, Ytr_w = Xw[:split_tr], Yw[:split_tr]\n",
    "Xv_w, Yv_w = Xw[split_tr:split_v], Yw[split_tr:split_v]\n",
    "Xte_w, Yte_w = Xw[split_v:], Yw[split_v:]\n",
    "\n",
    "# 5) Model + training\n",
    "input_dim = Xtr_w.shape[-1]\n",
    "mdl = TransformerRegressor(input_dim=input_dim, **cfg['model'])\n",
    "\n",
    "tr_ds = SeqDataset(Xtr_w, Ytr_w)\n",
    "va_ds = SeqDataset(Xv_w, Yv_w)\n",
    "te_ds = SeqDataset(Xte_w, Yte_w)\n",
    "\n",
    "state = train_transformer_cuda(\n",
    "    model=mdl,\n",
    "    train_ds=tr_ds,\n",
    "    val_ds=va_ds,\n",
    "    lr=cfg['lr'],\n",
    "    epochs=cfg['epochs'],\n",
    "    patience=cfg['patience'],\n",
    ")\n",
    "\n",
    "# Load best state, handling different return shapes\n",
    "best_state_dict = None\n",
    "if isinstance(state, dict):\n",
    "    best_state_dict = state.get('best_state_dict') or state.get('state_dict') or state.get('model_state_dict')\n",
    "elif isinstance(state, (list, tuple)):\n",
    "    for e in state:\n",
    "        if isinstance(e, dict) and any(k in e for k in ('best_state_dict','state_dict','model_state_dict')):\n",
    "            best_state_dict = e.get('best_state_dict') or e.get('state_dict') or e.get('model_state_dict')\n",
    "            break\n",
    "elif hasattr(state, 'best_state_dict'):\n",
    "    best_state_dict = getattr(state, 'best_state_dict')\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    mdl.load_state_dict(best_state_dict)\n",
    "mdl.eval()\n",
    "\n",
    "def predict_ds(ds, batch=512):\n",
    "    import torch\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=batch, shuffle=False)\n",
    "    preds = []\n",
    "    device = next(mdl.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device)\n",
    "            pr = mdl(xb).squeeze(-1).cpu().numpy()\n",
    "            preds.append(pr)\n",
    "    return np.concatenate(preds, axis=0)\n",
    "\n",
    "vpreds = predict_ds(va_ds)\n",
    "tpreds = predict_ds(te_ds)\n",
    "\n",
    "m_val = metrics(Yv_w, vpreds)\n",
    "m_test = metrics(Yte_w, tpreds)\n",
    "print('Val metrics:', m_val)\n",
    "print('Test metrics:', m_test)\n",
    "\n",
    "# 6) Backtest on val/test with compatibility wrapper\n",
    "\n",
    "def _compute_stats(net: pd.Series):\n",
    "    if net.std(ddof=0) > 0:\n",
    "        sharpe = (net.mean() / net.std(ddof=0)) * np.sqrt(252)\n",
    "    else:\n",
    "        sharpe = 0.0\n",
    "    dn = net[net < 0]\n",
    "    if dn.std(ddof=0) > 0:\n",
    "        sortino = (net.mean() / dn.std(ddof=0)) * np.sqrt(252)\n",
    "    else:\n",
    "        sortino = 0.0\n",
    "    eq = (1+net).cumprod()\n",
    "    rollmax = eq.cummax()\n",
    "    mdd = ((eq / rollmax) - 1.0).min()\n",
    "    years = max((eq.index[-1] - eq.index[0]).days / 365.25, 1e-9)\n",
    "    cagr = float(eq.iloc[-1]) ** (1/years) - 1.0 if len(eq) > 0 else 0.0\n",
    "    return {'sharpe': float(sharpe), 'sortino': float(sortino), 'mdd': float(mdd), 'cagr': float(cagr)}, eq\n",
    "\n",
    "\n",
    "def run_bt(prices, sig, cost):\n",
    "    # try multiple signatures/return types\n",
    "    for call in (\n",
    "        lambda: backtest(prices, sig, cost_bps=cost, slippage_bps=0),\n",
    "        lambda: backtest(prices, sig, tx_cost_bps=cost, slippage_bps=0),\n",
    "        lambda: backtest(prices, sig, cost),\n",
    "        lambda: backtest(prices, sig, cost, 0),\n",
    "    ):\n",
    "        try:\n",
    "            res = call()\n",
    "            # normalize outputs\n",
    "            if isinstance(res, tuple):\n",
    "                if len(res) == 3:\n",
    "                    # stats, net, equity\n",
    "                    return res[0], res[1], res[2]\n",
    "                if len(res) == 2:\n",
    "                    net, eq = res\n",
    "                    stats, _ = _compute_stats(net)\n",
    "                    return stats, net, eq\n",
    "            elif isinstance(res, dict):\n",
    "                # dict of stats only; recompute net/eq not available\n",
    "                return res, pd.Series(dtype=float), pd.Series(dtype=float)\n",
    "        except TypeError:\n",
    "            continue\n",
    "    # fallback: compute own backtest quickly\n",
    "    px = prices.astype(float)\n",
    "    rets = px.pct_change().dropna()\n",
    "    sig_s = pd.Series(sig, index=rets.index[-len(sig):]) if not isinstance(sig, pd.Series) else sig.reindex(rets.index).fillna(0)\n",
    "    pos = sig_s.shift(1).fillna(0).clip(-1,1)\n",
    "    net = pos * rets - sig_s.diff().abs().fillna(sig_s.abs()) * (cost/1e4)\n",
    "    stats, eq = _compute_stats(net)\n",
    "    return stats, net, eq\n",
    "\n",
    "val_price = price.iloc[-len(Yv_w)-len(Yte_w)-1: -len(Yte_w)-1]['close']\n",
    "test_price = price.iloc[-len(Yte_w)-1: -1]['close']\n",
    "\n",
    "val_sig = np.sign(vpreds)\n",
    "val_stats, val_net, val_eq = run_bt(val_price, val_sig, cfg['cost_bps'])\n",
    "print('Val stats:', val_stats)\n",
    "\n",
    "test_sig = np.sign(tpreds)\n",
    "test_stats, test_net, test_eq = run_bt(test_price, test_sig, cfg['cost_bps'])\n",
    "print('Test stats:', test_stats)\n",
    "\n",
    "# 7) Save artifacts + log\n",
    "artifacts = {\n",
    "    'config': cfg,\n",
    "    'hmm': hmm_model,\n",
    "    'scaler': scaler,\n",
    "    'feat_cols': feat_cols,\n",
    "}\n",
    "save_artifacts(out_dir, symbol, artifacts, model=mdl)\n",
    "log_run(symbol, seed=0, cfg=cfg, val_metrics=m_val, test_metrics=m_test, val_stats=val_stats, test_stats=test_stats, out_dir=out_dir)\n",
    "\n",
    "print('Done. Artifacts at', out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c832b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect train helper signature to adjust unified runner accordingly\n",
    "import inspect\n",
    "print('train_transformer_cuda signature:')\n",
    "print(inspect.signature(train_transformer_cuda))\n",
    "print('\\nDocstring:')\n",
    "print((train_transformer_cuda.__doc__ or '').strip() or '(no docstring)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48676d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified experiment quick launcher (auto device)\n",
    "try:\n",
    "    device = 'cuda' if (hasattr(__import__('torch'), 'cuda') and __import__('torch').cuda.is_available()) else 'cpu'\n",
    "    print('Using device:', device)\n",
    "except Exception:\n",
    "    device = 'cpu'\n",
    "    print('Using device:', device)\n",
    "\n",
    "# Expect the notebook to define `run_experiment` or similar unified runner.\n",
    "if 'run_experiment' in globals():\n",
    "    results = run_experiment(device=device)\n",
    "    print('Experiment done. Keys:', list(results.keys()) if isinstance(results, dict) else type(results))\n",
    "else:\n",
    "    print('run_experiment not found. Scroll up to run the setup cells first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4a4a6",
   "metadata": {},
   "source": [
    "## Transformer sequence preparation\n",
    "\n",
    "The following builds fixed-length sequences from `trans_df` with scaling and a train/val/test split, ready for transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc942a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build transformer-ready sequences from trans_df\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SEQ_LEN = 64  # configurable\n",
    "TARGET_COL = \"close\"  # example supervised target\n",
    "\n",
    "# Select features (you can extend this list)\n",
    "feature_cols = [c for c in trans_df.columns if c not in [\"open\",\"high\",\"low\",\"close\",\"volume\"]]\n",
    "# Include price if needed for context\n",
    "feature_cols = [\"close\"] + feature_cols\n",
    "\n",
    "# Scale features (fit on train later; here we fit on all for a quick demo)\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(trans_df[feature_cols].values)\n",
    "\n",
    "# Simple target: next-period return over close\n",
    "ret = trans_df[\"close\"].pct_change().shift(-1)\n",
    "ret = ret.iloc[SEQ_LEN-1:-1]  # align with sequences\n",
    "\n",
    "# Build sequences\n",
    "X_seq = []\n",
    "for i in range(len(features) - SEQ_LEN):\n",
    "    X_seq.append(features[i:i+SEQ_LEN])\n",
    "X_seq = np.array(X_seq)\n",
    "\n",
    "# Align target to sequences start\n",
    "# After building sequences starting at 0..(N-SEQ_LEN-1), target indices start at SEQ_LEN-1\n",
    "y = ret.values\n",
    "\n",
    "# Train/Val/Test split (70/15/15)\n",
    "N = len(X_seq)\n",
    "train_end = int(0.7 * N)\n",
    "val_end = int(0.85 * N)\n",
    "X_train, y_train = X_seq[:train_end], y[:train_end]\n",
    "X_val, y_val = X_seq[train_end:val_end], y[train_end:val_end]\n",
    "X_test, y_test = X_seq[val_end:], y[val_end:]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train\", X_train.shape, \"y_train\", y_train.shape)\n",
    "print(\"X_val  \", X_val.shape, \"y_val  \", y_val.shape)\n",
    "print(\"X_test \", X_test.shape, \"y_test \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short-term HMM-driven trading signals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SIG_CFG = {\n",
    "    'min_prob': 0.55,\n",
    "    'fast_ma': 5,\n",
    "    'slow_ma': 20,\n",
    "    'cooldown': 3,\n",
    "    'fee_bps': 0.5,   # per trade (enter or flip) fee in bps\n",
    "    'slip_bps': 1.0,  # per trade slippage in bps\n",
    "}\n",
    "\n",
    "if 'trans_df' not in globals():\n",
    "    raise RuntimeError(\"trans_df not found. Run data fetch and HMM pipeline cells first.\")\n",
    "\n",
    "sdf = trans_df.copy()\n",
    "sdf['ma_fast'] = sdf['close'].rolling(SIG_CFG['fast_ma'], min_periods=1).mean()\n",
    "sdf['ma_slow'] = sdf['close'].rolling(SIG_CFG['slow_ma'], min_periods=1).mean()\n",
    "sdf['ret1'] = sdf['close'].pct_change().fillna(0.0)\n",
    "\n",
    "# choose best state by max probability if available, else by one-hot\n",
    "prob_cols = [c for c in sdf.columns if c.startswith('state_p')]\n",
    "onehot_cols = [c for c in sdf.columns if c.startswith('state_') and not c.startswith('state_p') and c != 'hmm_state']\n",
    "if prob_cols:\n",
    "    sdf['best_state'] = sdf[prob_cols].idxmax(axis=1).str.extract(r'(\\d+)').astype(int)\n",
    "    sdf['best_prob'] = sdf[prob_cols].max(axis=1)\n",
    "else:\n",
    "    sdf['best_state'] = sdf[onehot_cols].idxmax(axis=1).str.extract(r'(\\d+)').astype(int)\n",
    "    sdf['best_prob'] = 1.0\n",
    "\n",
    "# Estimate directional bias per state from next-bar return\n",
    "next_ret = sdf['ret1'].shift(-1)\n",
    "bias = next_ret.groupby(sdf['hmm_state']).mean().to_dict()\n",
    "sdf['state_dir'] = sdf['best_state'].map(lambda s: np.sign(bias.get(int(s), 0.0)))\n",
    "\n",
    "sdf['long_signal'] = (sdf['best_prob'] >= SIG_CFG['min_prob']) & (sdf['state_dir'] > 0) & (sdf['ma_fast'] > sdf['ma_slow'])\n",
    "sdf['short_signal'] = (sdf['best_prob'] >= SIG_CFG['min_prob']) & (sdf['state_dir'] < 0) & (sdf['ma_fast'] < sdf['ma_slow'])\n",
    "\n",
    "# turn signals into positions with cooldown\n",
    "pos = []\n",
    "last = 0\n",
    "cd = 0\n",
    "for _, r in sdf.iterrows():\n",
    "    if cd > 0:\n",
    "        pos.append(last)\n",
    "        cd -= 1\n",
    "        continue\n",
    "    p = last\n",
    "    if r['long_signal'] and last <= 0:\n",
    "        p = 1\n",
    "        cd = SIG_CFG['cooldown']\n",
    "    elif r['short_signal'] and last >= 0:\n",
    "        p = -1\n",
    "        cd = SIG_CFG['cooldown']\n",
    "    pos.append(p)\n",
    "    last = p\n",
    "\n",
    "sdf['position'] = pd.Series(pos, index=sdf.index)\n",
    "\n",
    "# returns and trading costs\n",
    "sdf['strategy_ret'] = sdf['position'].shift(1).fillna(0) * sdf['ret1']\n",
    "turn = (sdf['position'].diff().fillna(0) != 0).astype(float)\n",
    "cost_per_trade = (SIG_CFG['fee_bps'] + SIG_CFG['slip_bps']) / 1e4\n",
    "sdf['strategy_ret'] = sdf['strategy_ret'] - turn * cost_per_trade\n",
    "\n",
    "ann_factor = 252 if TIMEFRAME.endswith('d') else 24*60\n",
    "cum = (1 + sdf['strategy_ret'].fillna(0)).cumprod()\n",
    "sharpe = sdf['strategy_ret'].mean() / (sdf['strategy_ret'].std() + 1e-12) * np.sqrt(ann_factor)\n",
    "winrate = (sdf['strategy_ret'] > 0).mean()\n",
    "\n",
    "print(\"Short-term signals ready.\")\n",
    "print(f\"Final equity multiple: {float(cum.iloc[-1]) if len(cum) else 1.0:.3f}\")\n",
    "print(f\"Sharpe (approx): {sharpe:.2f}\")\n",
    "print(f\"Win rate: {winrate:.2%}\")\n",
    "\n",
    "sdf[['close','ma_fast','ma_slow','best_state','best_prob','state_dir','long_signal','short_signal','position','strategy_ret']].tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795a476",
   "metadata": {},
   "source": [
    "# Optuna hyperparameter optimization\n",
    "\n",
    "This section tunes short-term trading parameters using Optuna. It searches over HMM and signal parameters and maximizes validation Sharpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4dc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/import Optuna and define optimization objective\n",
    "try:\n",
    "    import optuna\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'optuna'])\n",
    "    import optuna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "FEE_BPS = 0.5\n",
    "SLIP_BPS = 1.0\n",
    "\n",
    "\n",
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out['ret1'] = df['close'].pct_change()\n",
    "    out['log_ret1'] = np.log(df['close']).diff()\n",
    "    out['hl_range'] = (df['high'] - df['low']) / df['close']\n",
    "    out['vol10'] = out['ret1'].rolling(10).std()\n",
    "    out['vol20'] = out['ret1'].rolling(20).std()\n",
    "    delta = df['close'].diff()\n",
    "    up = np.where(delta > 0, delta, 0.0)\n",
    "    down = np.where(delta < 0, -delta, 0.0)\n",
    "    roll_up = pd.Series(up, index=df.index).rolling(14).mean()\n",
    "    roll_down = pd.Series(down, index=df.index).rolling(14).mean()\n",
    "    rs = roll_up / (roll_down + 1e-12)\n",
    "    out['rsi14'] = 100 - (100 / (1 + rs))\n",
    "    return out.dropna()\n",
    "\n",
    "\n",
    "def fit_hmm_and_states(feat: pd.DataFrame, n_states: int, cov_type: str, random_state: int) -> tuple[GaussianHMM, np.ndarray, np.ndarray]:\n",
    "    X = feat.values\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    hmm = GaussianHMM(n_components=n_states, covariance_type=cov_type, n_iter=200, random_state=random_state, verbose=False)\n",
    "    hmm.fit(Xs)\n",
    "    states = hmm.predict(Xs)\n",
    "    probs = hmm.predict_proba(Xs)\n",
    "    return hmm, states, probs\n",
    "\n",
    "\n",
    "def build_signals(df: pd.DataFrame, states: np.ndarray, probs: np.ndarray, min_prob: float, fast_ma: int, slow_ma: int, cooldown: int) -> pd.DataFrame:\n",
    "    sdf = pd.DataFrame(index=df.index)\n",
    "    sdf['close'] = df['close']\n",
    "    sdf['ma_fast'] = df['close'].rolling(fast_ma, min_periods=1).mean()\n",
    "    sdf['ma_slow'] = df['close'].rolling(slow_ma, min_periods=1).mean()\n",
    "    sdf['ret1'] = df['close'].pct_change().fillna(0.0)\n",
    "    best_idx = probs.argmax(axis=1)\n",
    "    best_prob = probs.max(axis=1)\n",
    "    sdf['best_state'] = best_idx\n",
    "    sdf['best_prob'] = best_prob\n",
    "    next_ret = sdf['ret1'].shift(-1)\n",
    "    bias = next_ret.groupby(states).mean().to_dict()\n",
    "    sdf['state_dir'] = [np.sign(bias.get(int(s), 0.0)) for s in best_idx]\n",
    "    sdf['long_signal'] = (sdf['best_prob'] >= min_prob) & (sdf['state_dir'] > 0) & (sdf['ma_fast'] > sdf['ma_slow'])\n",
    "    sdf['short_signal'] = (sdf['best_prob'] >= min_prob) & (sdf['state_dir'] < 0) & (sdf['ma_fast'] < sdf['ma_slow'])\n",
    "    pos, last, cd = [], 0, 0\n",
    "    for ls, ss in zip(sdf['long_signal'], sdf['short_signal']):\n",
    "        if cd > 0:\n",
    "            pos.append(last)\n",
    "            cd -= 1\n",
    "            continue\n",
    "        p = last\n",
    "        if ls and last <= 0:\n",
    "            p = 1; cd = cooldown\n",
    "        elif ss and last >= 0:\n",
    "            p = -1; cd = cooldown\n",
    "        pos.append(p)\n",
    "        last = p\n",
    "    sdf['position'] = pos\n",
    "    # returns and costs\n",
    "    sdf['strategy_ret'] = pd.Series(sdf['position']).shift(1).fillna(0) * sdf['ret1']\n",
    "    turn = (pd.Series(sdf['position']).diff().fillna(0) != 0).astype(float)\n",
    "    cost = (FEE_BPS + SLIP_BPS) / 1e4\n",
    "    sdf['strategy_ret'] = sdf['strategy_ret'] - turn * cost\n",
    "    return sdf\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    n_states = trial.suggest_int('n_states', 3, 7)\n",
    "    cov_type = trial.suggest_categorical('cov_type', ['full', 'diag'])\n",
    "    min_prob = trial.suggest_float('min_prob', 0.5, 0.75)\n",
    "    fast_ma = trial.suggest_int('fast_ma', 3, 12)\n",
    "    slow_ma = trial.suggest_int('slow_ma', 10, 40)\n",
    "    cooldown = trial.suggest_int('cooldown', 0, 5)\n",
    "\n",
    "    df = df_raw.copy()\n",
    "    feat = build_features(df)\n",
    "    align = df.loc[feat.index]\n",
    "\n",
    "    hmm, states, probs = fit_hmm_and_states(feat, n_states, cov_type, RANDOM_STATE)\n",
    "    sig = build_signals(align, states, probs, min_prob, fast_ma, slow_ma, cooldown)\n",
    "\n",
    "    n = len(sig)\n",
    "    if n < 200:\n",
    "        return -1e3\n",
    "    split = int(n * 0.8)\n",
    "    val = sig.iloc[split:]\n",
    "\n",
    "    ann_factor = 252 if TIMEFRAME.endswith('d') else 24*60\n",
    "    sharpe = val['strategy_ret'].mean() / (val['strategy_ret'].std() + 1e-12) * np.sqrt(ann_factor)\n",
    "    turns = (val['position'].diff().abs() > 0).sum()\n",
    "    sharpe_adj = float(sharpe - 0.01 * max(0, 20 - turns))\n",
    "    return sharpe_adj\n",
    "\n",
    "print(\"Optuna ready. Call the study cell to run optimization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed46fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna study and apply best parameters\n",
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "print(\"Best value (Sharpe_adj):\", study.best_value)\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Refit with best params and build final signals\n",
    "best = study.best_params\n",
    "\n",
    "feat_all = build_features(df_raw)\n",
    "align_all = df_raw.loc[feat_all.index]\n",
    "hmm_b, states_b, probs_b = fit_hmm_and_states(\n",
    "    feat_all,\n",
    "    n_states=best['n_states'],\n",
    "    cov_type=best['cov_type'],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "sig_best = build_signals(\n",
    "    align_all, states_b, probs_b,\n",
    "    min_prob=best['min_prob'],\n",
    "    fast_ma=best['fast_ma'],\n",
    "    slow_ma=best['slow_ma'],\n",
    "    cooldown=best['cooldown'],\n",
    ")\n",
    "\n",
    "ann_factor = 252 if TIMEFRAME.endswith('d') else 24*60\n",
    "cum = (1 + sig_best['strategy_ret'].fillna(0)).cumprod()\n",
    "sharpe = sig_best['strategy_ret'].mean() / (sig_best['strategy_ret'].std() + 1e-12) * np.sqrt(ann_factor)\n",
    "winrate = (sig_best['strategy_ret'] > 0).mean()\n",
    "print(f\"Final equity multiple (all data): {float(cum.iloc[-1]) if len(cum) else 1.0:.3f}\")\n",
    "print(f\"Sharpe (approx): {sharpe:.2f}\")\n",
    "print(f\"Win rate: {winrate:.2%}\")\n",
    "\n",
    "sig_best[['close','ma_fast','ma_slow','best_state','best_prob','state_dir','long_signal','short_signal','position','strategy_ret']].tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best params and signals to disk\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('data/analysis/hmm_transformer')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "params_path = out_dir / 'best_params.json'\n",
    "signals_csv = out_dir / f'signals_{SYMBOL.replace(\"/\",\"_\")}_{TIMEFRAME}.csv'\n",
    "signals_parquet = out_dir / f'signals_{SYMBOL.replace(\"/\",\"_\")}_{TIMEFRAME}.parquet'\n",
    "\n",
    "best_params = study.best_params if 'study' in globals() else {}\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "if 'sig_best' in globals():\n",
    "    sig_best.to_csv(signals_csv)\n",
    "    try:\n",
    "        sig_best.to_parquet(signals_parquet)\n",
    "    except Exception as e:\n",
    "        print(f\"Parquet save skipped: {e}\")\n",
    "    print(f\"Saved params -> {params_path}\")\n",
    "    print(f\"Saved signals CSV -> {signals_csv}\")\n",
    "else:\n",
    "    print(\"sig_best not found. Run the Optuna study cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64722722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Optuna study first to get best_params.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Configuration for MC simulations\n",
    "MC_SIMS = 1000  # Number of simulations\n",
    "MC_HORIZON = 252  # Days to simulate ahead (e.g., 1 year of trading days)\n",
    "APPROVAL_THRESHOLD_SHARPE = 1.0  # Minimum average Sharpe for approval\n",
    "VOL_SCALE = 1.0  # Optional volatility multiplier for simulations (e.g., stress test with higher vol)\n",
    "\n",
    "def simulate_path(historical_returns, length, mu=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Simulate a price path using Geometric Brownian Motion (GBM) fitted to historical returns.\n",
    "    - historical_returns: pd.Series of log returns\n",
    "    - length: number of steps to simulate\n",
    "    - mu, sigma: optional override for mean and std (else fitted)\n",
    "    \"\"\"\n",
    "    log_rets = np.log(1 + historical_returns).dropna()\n",
    "    mu = mu or log_rets.mean()\n",
    "    sigma = (sigma or log_rets.std()) * VOL_SCALE\n",
    "    random_walk = np.random.normal(mu, sigma, length)\n",
    "    prices = np.exp(np.cumsum(random_walk))\n",
    "    prices = prices / prices[0] * historical_returns.index.get_level_values('close')[-1] if hasattr(historical_returns.index, 'get_level_values') else historical_returns.iloc[-1]\n",
    "    return pd.Series(prices)\n",
    "\n",
    "def extend_df_with_sim(df, sim_prices):\n",
    "    \"\"\"\n",
    "    Append simulated prices to historical df, assuming O=H=L=C, volume=0 for sim.\n",
    "    Index with future dates.\n",
    "    \"\"\"\n",
    "    last_date = df.index[-1]\n",
    "    future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=len(sim_prices), freq='B')  # Business days\n",
    "    sim_df = pd.DataFrame({\n",
    "        'open': sim_prices,\n",
    "        'high': sim_prices,\n",
    "        'low': sim_prices,\n",
    "        'close': sim_prices,\n",
    "        'volume': 0\n",
    "    }, index=future_dates)\n",
    "    extended = pd.concat([df, sim_df])\n",
    "    return extended\n",
    "\n",
    "def run_mc_strategy(best_params, historical_df, mc_sims=MC_SIMS, mc_horizon=MC_HORIZON):\n",
    "    \"\"\"\n",
    "    Run Monte Carlo on the optimized strategy.\n",
    "    - best_params: dict from Optuna\n",
    "    - historical_df: df_raw or trans_df with OHLCV\n",
    "    - Returns list of Sharpes on simulated parts\n",
    "    \"\"\"\n",
    "    historical_returns = historical_df['close'].pct_change().dropna()\n",
    "    sharpes = []\n",
    "    for _ in tqdm(range(mc_sims)):\n",
    "        sim_prices = simulate_path(historical_returns, mc_horizon)\n",
    "        extended_df = extend_df_with_sim(historical_df, sim_prices)\n",
    "        \n",
    "        # Recompute features on extended\n",
    "        feat_ext = build_features(extended_df)\n",
    "        \n",
    "        # Fit HMM on historical only, predict on full\n",
    "        feat_hist = feat_ext.iloc[:len(historical_df)]\n",
    "        hmm, _, _ = fit_hmm_and_states(feat_hist, best_params['n_states'], best_params['cov_type'], RANDOM_STATE)\n",
    "        _, probs_ext = hmm.predict_proba(feat_ext.values), None  # Wait, fit_hmm_and_states returns hmm, states, probs\n",
    "        # Correct: use the function to get probs on extended\n",
    "        _, _, probs_ext = fit_hmm_and_states(feat_ext, best_params['n_states'], best_params['cov_type'], RANDOM_STATE)  # Refit on extended? No, to avoid lookahead, fit on hist, predict on ext\n",
    "        # Better:\n",
    "        hmm_hist, _, _ = fit_hmm_and_states(feat_hist, best_params['n_states'], best_params['cov_type'], RANDOM_STATE)\n",
    "        states_ext = hmm_hist.predict(feat_ext.values)\n",
    "        probs_ext = hmm_hist.predict_proba(feat_ext.values)\n",
    "        \n",
    "        # Build signals on extended\n",
    "        sig_ext = build_signals(extended_df.loc[feat_ext.index], states_ext, probs_ext, **{k: best_params[k] for k in ['min_prob', 'fast_ma', 'slow_ma', 'cooldown']})\n",
    "        \n",
    "        # Extract sim part returns\n",
    "        sim_ret = sig_ext['strategy_ret'].iloc[-mc_horizon:]\n",
    "        ann_factor = 252  # Assume daily\n",
    "        sharpe = sim_ret.mean() / (sim_ret.std() + 1e-12) * np.sqrt(ann_factor)\n",
    "        sharpes.append(sharpe)\n",
    "    return sharpes\n",
    "\n",
    "# Run MC after Optuna\n",
    "if 'study' in globals() and study.best_params:\n",
    "    sharpes = run_mc_strategy(study.best_params, df_raw, mc_sims=MC_SIMS, mc_horizon=MC_HORIZON)\n",
    "    mean_sharpe = np.mean(sharpes)\n",
    "    std_sharpe = np.std(sharpes)\n",
    "    print(f\"MC Results: Mean Sharpe = {mean_sharpe:.2f}, Std = {std_sharpe:.2f}\")\n",
    "    if mean_sharpe > APPROVAL_THRESHOLD_SHARPE:\n",
    "        print(\"Strategy APPROVED for live use.\")\n",
    "    else:\n",
    "        print(\"Strategy NOT approved.\")\n",
    "else:\n",
    "    print(\"Run Optuna study first to get best_params.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83ca6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sharpe': 0.11723838096787674, 'mdd': -0.01844777291690991, 'equity_final': 1.0093616589981136}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity backtest: long best-mean-return state, short worst-mean-return state\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Derive daily pnl from states inferred earlier\n",
    "price = features_df['close']\n",
    "ret = price.pct_change().fillna(0.0)\n",
    "\n",
    "# Compute average return per state to label bull/bear regimes\n",
    "state_means = pd.Series(states, index=features_df.index).to_frame('state').join(ret.rename('ret'))\\\n",
    "    .groupby('state')['ret'].mean().sort_values()\n",
    "low_s = int(state_means.index[0])\n",
    "high_s = int(state_means.index[-1])\n",
    "\n",
    "sig = pd.Series(0, index=features_df.index)\n",
    "sig[features_df.index[states == high_s]] = 1\n",
    "sig[features_df.index[states == low_s]] = -1\n",
    "\n",
    "pos = sig.shift(1).fillna(0.0)\n",
    "net = (pos * ret)\n",
    "\n",
    "equity = (1.0 + net).cumprod()\n",
    "\n",
    "sharpe = (net.mean() / (net.std() + 1e-12)) * np.sqrt(252)\n",
    "rollmax = equity.cummax()\n",
    "mdd = float(((equity / rollmax) - 1.0).min())\n",
    "print({'sharpe': float(sharpe), 'mdd': mdd, 'equity_final': float(equity.iloc[-1])})\n",
    "\n",
    "ax = equity.plot(figsize=(10,3), title='Sanity Backtest Equity (HMM state gating)')\n",
    "ax.figure.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fks_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
